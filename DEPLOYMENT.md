# MediaCrawler API Server éƒ¨ç½²æŒ‡å—

## ğŸ“‹ éƒ¨ç½²æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»äº† MediaCrawler API Server çš„å„ç§éƒ¨ç½²æ–¹å¼ï¼ŒåŒ…æ‹¬å¼€å‘ç¯å¢ƒã€ç”Ÿäº§ç¯å¢ƒã€Docker å®¹å™¨åŒ–éƒ¨ç½²ä»¥åŠäº‘æœåŠ¡éƒ¨ç½²ç­‰ã€‚

## ğŸ› ï¸ ç³»ç»Ÿè¦æ±‚

### æœ€ä½é…ç½®
- **CPU**: 2 æ ¸å¿ƒ
- **å†…å­˜**: 4GB RAM
- **å­˜å‚¨**: 20GB å¯ç”¨ç©ºé—´
- **æ“ä½œç³»ç»Ÿ**: Ubuntu 20.04+, CentOS 8+, macOS 10.15+, Windows 10+
- **Python**: 3.8+

### æ¨èé…ç½®
- **CPU**: 4 æ ¸å¿ƒ æˆ–æ›´å¤š
- **å†…å­˜**: 8GB RAM æˆ–æ›´å¤š
- **å­˜å‚¨**: 50GB SSD
- **ç½‘ç»œ**: ç¨³å®šçš„äº’è”ç½‘è¿æ¥

### è½¯ä»¶ä¾èµ–
- Python 3.8+
- pip
- Git
- Chrome/Chromium æµè§ˆå™¨
- Redis (å¯é€‰ï¼Œç”¨äºç¼“å­˜)
- PostgreSQL/MySQL (å¯é€‰ï¼Œç”¨äºç”Ÿäº§ç¯å¢ƒ)

## ğŸš€ å¿«é€Ÿéƒ¨ç½²

### æ–¹å¼ä¸€ï¼šæœ¬åœ°å¼€å‘éƒ¨ç½²

1. **å…‹éš†é¡¹ç›®**
```bash
git clone https://github.com/your-repo/MediaCrawler-ApiServer.git
cd MediaCrawler-ApiServer
```

2. **åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ**
```bash
python -m venv venv

# Linux/macOS
source venv/bin/activate

# Windows
venv\Scripts\activate
```

3. **å®‰è£…ä¾èµ–**
```bash
pip install -r requirements.txt
```

4. **é…ç½®ç¯å¢ƒå˜é‡**
```bash
cp config.env.example .env
```

ç¼–è¾‘ `.env` æ–‡ä»¶ï¼š
```bash
# åŸºæœ¬é…ç½®
APP_NAME=MediaCrawler API Server
APP_VERSION=1.0.0
DEBUG=true
LOG_LEVEL=INFO

# æ•°æ®åº“é…ç½® (å¯é€‰)
DATABASE_URL=sqlite:///./data/app.db

# Supabase é…ç½® (å¯é€‰)
SUPABASE_URL=your_supabase_url
SUPABASE_KEY=your_supabase_key

# çˆ¬è™«é»˜è®¤é…ç½®
DEFAULT_HEADLESS=true
DEFAULT_ENABLE_PROXY=false
DEFAULT_MAX_RETRIES=3
DEFAULT_TIMEOUT=30

# ä»£ç†é…ç½® (å¯é€‰)
DEFAULT_PROXY_PROVIDER=kuaidaili
KUAIDAILI_USERNAME=your_username
KUAIDAILI_PASSWORD=your_password
```

5. **åˆ›å»ºæ•°æ®ç›®å½•**
```bash
mkdir -p data/xhs/json data/xhs/csv
mkdir -p logs
```

6. **å¯åŠ¨æœåŠ¡**
```bash
python -m uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

7. **éªŒè¯éƒ¨ç½²**
```bash
curl http://localhost:8000/
curl http://localhost:8000/api/v1/data/health
```

### æ–¹å¼äºŒï¼šDocker éƒ¨ç½²

1. **åˆ›å»º Dockerfile**
```dockerfile
FROM python:3.9-slim

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    chromium \
    chromium-driver \
    fonts-liberation \
    libappindicator3-1 \
    libasound2 \
    libatk-bridge2.0-0 \
    libnspr4 \
    libnss3 \
    libx11-xcb1 \
    libxcomposite1 \
    libxcursor1 \
    libxdamage1 \
    libxi6 \
    libxrandr2 \
    libxss1 \
    libxtst6 \
    xdg-utils \
    && rm -rf /var/lib/apt/lists/*

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å¤åˆ¶å¹¶å®‰è£…Pythonä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# åˆ›å»ºæ•°æ®ç›®å½•
RUN mkdir -p data/xhs/json data/xhs/csv logs

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PYTHONPATH=/app
ENV CHROME_BIN=/usr/bin/chromium
ENV CHROME_DRIVER=/usr/bin/chromedriver

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/api/v1/data/health || exit 1

# å¯åŠ¨å‘½ä»¤
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

2. **åˆ›å»º docker-compose.yml**
```yaml
version: '3.8'

services:
  mediacrawler-api:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
      - ./config.env:/app/.env
    environment:
      - PYTHONPATH=/app
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/data/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # å¯é€‰ï¼šRedis ç¼“å­˜
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped

  # å¯é€‰ï¼šPostgreSQL æ•°æ®åº“
  postgres:
    image: postgres:15-alpine
    environment:
      POSTGRES_DB: mediacrawler
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: your_password
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    restart: unless-stopped

volumes:
  redis_data:
  postgres_data:
```

3. **æ„å»ºå¹¶å¯åŠ¨**
```bash
docker-compose up -d --build
```

4. **æŸ¥çœ‹æ—¥å¿—**
```bash
docker-compose logs -f mediacrawler-api
```

## ğŸŒ ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

### Nginx åå‘ä»£ç†é…ç½®

1. **å®‰è£… Nginx**
```bash
# Ubuntu/Debian
sudo apt update
sudo apt install nginx

# CentOS/RHEL
sudo yum install nginx
```

2. **åˆ›å»ºé…ç½®æ–‡ä»¶** `/etc/nginx/sites-available/mediacrawler-api`
```nginx
upstream mediacrawler_backend {
    server 127.0.0.1:8000;
    # å¦‚æœæœ‰å¤šä¸ªå®ä¾‹ï¼Œå¯ä»¥æ·»åŠ è´Ÿè½½å‡è¡¡
    # server 127.0.0.1:8001;
    # server 127.0.0.1:8002;
}

server {
    listen 80;
    server_name your-domain.com www.your-domain.com;

    # é‡å®šå‘åˆ° HTTPS
    return 301 https://$server_name$request_uri;
}

server {
    listen 443 ssl http2;
    server_name your-domain.com www.your-domain.com;

    # SSL è¯ä¹¦é…ç½®
    ssl_certificate /path/to/your/certificate.crt;
    ssl_certificate_key /path/to/your/private.key;
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers ECDHE-RSA-AES256-GCM-SHA512:DHE-RSA-AES256-GCM-SHA512:ECDHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES256-GCM-SHA384;
    ssl_prefer_server_ciphers off;

    # å®‰å…¨å¤´
    add_header X-Frame-Options DENY;
    add_header X-Content-Type-Options nosniff;
    add_header X-XSS-Protection "1; mode=block";
    add_header Strict-Transport-Security "max-age=63072000; includeSubDomains; preload";

    # å®¢æˆ·ç«¯ä¸Šä¼ é™åˆ¶
    client_max_body_size 100M;

    # æ—¥å¿—é…ç½®
    access_log /var/log/nginx/mediacrawler_access.log;
    error_log /var/log/nginx/mediacrawler_error.log;

    # API ä»£ç†
    location / {
        proxy_pass http://mediacrawler_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # WebSocket æ”¯æŒ
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        
        # è¶…æ—¶è®¾ç½®
        proxy_connect_timeout 60s;
        proxy_send_timeout 60s;
        proxy_read_timeout 60s;
    }

    # é™æ€æ–‡ä»¶ç¼“å­˜
    location ~* \.(jpg|jpeg|png|gif|ico|css|js)$ {
        expires 1y;
        add_header Cache-Control "public, immutable";
    }

    # å¥åº·æ£€æŸ¥
    location /health {
        access_log off;
        proxy_pass http://mediacrawler_backend/api/v1/data/health;
    }
}
```

3. **å¯ç”¨é…ç½®**
```bash
sudo ln -s /etc/nginx/sites-available/mediacrawler-api /etc/nginx/sites-enabled/
sudo nginx -t
sudo systemctl reload nginx
```

### Systemd æœåŠ¡é…ç½®

1. **åˆ›å»ºæœåŠ¡æ–‡ä»¶** `/etc/systemd/system/mediacrawler-api.service`
```ini
[Unit]
Description=MediaCrawler API Server
After=network.target

[Service]
Type=exec
User=www-data
Group=www-data
WorkingDirectory=/opt/mediacrawler-api
Environment=PATH=/opt/mediacrawler-api/venv/bin
EnvironmentFile=/opt/mediacrawler-api/.env
ExecStart=/opt/mediacrawler-api/venv/bin/uvicorn app.main:app --host 127.0.0.1 --port 8000 --workers 4
ExecReload=/bin/kill -HUP $MAINPID
Restart=always
RestartSec=10
StandardOutput=journal
StandardError=journal
SyslogIdentifier=mediacrawler-api

# å®‰å…¨è®¾ç½®
NoNewPrivileges=yes
PrivateTmp=yes
ProtectSystem=strict
ReadWritePaths=/opt/mediacrawler-api/data /opt/mediacrawler-api/logs
ProtectHome=yes

[Install]
WantedBy=multi-user.target
```

2. **éƒ¨ç½²åº”ç”¨ä»£ç **
```bash
sudo mkdir -p /opt/mediacrawler-api
sudo chown www-data:www-data /opt/mediacrawler-api

# åˆ‡æ¢åˆ° www-data ç”¨æˆ·
sudo -u www-data -s

# å…‹éš†ä»£ç 
cd /opt/mediacrawler-api
git clone https://github.com/your-repo/MediaCrawler-ApiServer.git .

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# é…ç½®ç¯å¢ƒå˜é‡
cp config.env.example .env
# ç¼–è¾‘ .env æ–‡ä»¶...

# åˆ›å»ºæ•°æ®ç›®å½•
mkdir -p data/xhs/json data/xhs/csv logs
```

3. **å¯åŠ¨æœåŠ¡**
```bash
sudo systemctl daemon-reload
sudo systemctl enable mediacrawler-api
sudo systemctl start mediacrawler-api
sudo systemctl status mediacrawler-api
```

### ç›‘æ§å’Œæ—¥å¿—

1. **æ—¥å¿—è½®è½¬** `/etc/logrotate.d/mediacrawler-api`
```
/opt/mediacrawler-api/logs/*.log {
    daily
    missingok
    rotate 30
    compress
    delaycompress
    notifempty
    postrotate
        systemctl reload mediacrawler-api
    endscript
}
```

2. **ç›‘æ§è„šæœ¬** `monitor.sh`
```bash
#!/bin/bash

# å¥åº·æ£€æŸ¥
health_check() {
    response=$(curl -s -o /dev/null -w "%{http_code}" http://localhost:8000/api/v1/data/health)
    if [ "$response" -eq 200 ]; then
        echo "$(date): Health check passed"
        return 0
    else
        echo "$(date): Health check failed with code $response"
        return 1
    fi
}

# å†…å­˜ä½¿ç”¨æ£€æŸ¥
memory_check() {
    memory_usage=$(ps -o pid,ppid,user,%mem,%cpu,cmd -C python | grep uvicorn | awk '{sum+=$4} END {print sum}')
    if [ $(echo "$memory_usage > 80" | bc) -eq 1 ]; then
        echo "$(date): High memory usage: $memory_usage%"
        # å‘é€å‘Šè­¦...
    fi
}

# ç£ç›˜ç©ºé—´æ£€æŸ¥
disk_check() {
    disk_usage=$(df /opt/mediacrawler-api | awk 'NR==2 {print $5}' | sed 's/%//')
    if [ "$disk_usage" -gt 85 ]; then
        echo "$(date): High disk usage: $disk_usage%"
        # æ¸…ç†æ—§æ—¥å¿—æˆ–æ•°æ®...
    fi
}

# æ‰§è¡Œæ£€æŸ¥
health_check || exit 1
memory_check
disk_check
```

3. **Crontab å®šæ—¶ä»»åŠ¡**
```bash
# ç¼–è¾‘å®šæ—¶ä»»åŠ¡
crontab -e

# æ·»åŠ ç›‘æ§ä»»åŠ¡
*/5 * * * * /opt/mediacrawler-api/monitor.sh >> /var/log/mediacrawler-monitor.log 2>&1

# æ¯æ—¥æ•°æ®å¤‡ä»½
0 2 * * * /opt/mediacrawler-api/backup.sh >> /var/log/mediacrawler-backup.log 2>&1
```

## â˜ï¸ äº‘æœåŠ¡éƒ¨ç½²

### AWS ECS éƒ¨ç½²

1. **åˆ›å»º ECS ä»»åŠ¡å®šä¹‰**
```json
{
  "family": "mediacrawler-api",
  "networkMode": "awsvpc",
  "requiresCompatibilities": ["FARGATE"],
  "cpu": "1024",
  "memory": "2048",
  "executionRoleArn": "arn:aws:iam::YOUR_ACCOUNT:role/ecsTaskExecutionRole",
  "containerDefinitions": [
    {
      "name": "mediacrawler-api",
      "image": "your-account.dkr.ecr.region.amazonaws.com/mediacrawler-api:latest",
      "portMappings": [
        {
          "containerPort": 8000,
          "protocol": "tcp"
        }
      ],
      "environment": [
        {
          "name": "DATABASE_URL",
          "value": "postgresql://user:pass@rds-endpoint:5432/db"
        }
      ],
      "logConfiguration": {
        "logDriver": "awslogs",
        "options": {
          "awslogs-group": "/ecs/mediacrawler-api",
          "awslogs-region": "us-east-1",
          "awslogs-stream-prefix": "ecs"
        }
      },
      "healthCheck": {
        "command": [
          "CMD-SHELL",
          "curl -f http://localhost:8000/api/v1/data/health || exit 1"
        ],
        "interval": 30,
        "timeout": 5,
        "retries": 3,
        "startPeriod": 60
      }
    }
  ]
}
```

2. **åˆ›å»º ECS æœåŠ¡**
```bash
aws ecs create-service \
  --cluster your-cluster \
  --service-name mediacrawler-api \
  --task-definition mediacrawler-api:1 \
  --desired-count 2 \
  --launch-type FARGATE \
  --network-configuration "awsvpcConfiguration={subnets=[subnet-12345],securityGroups=[sg-12345],assignPublicIp=ENABLED}" \
  --load-balancers "targetGroupArn=arn:aws:elasticloadbalancing:...,containerName=mediacrawler-api,containerPort=8000"
```

### Google Cloud Run éƒ¨ç½²

1. **æ„å»ºé•œåƒå¹¶æ¨é€åˆ° GCR**
```bash
# æ„å»ºé•œåƒ
docker build -t gcr.io/YOUR_PROJECT_ID/mediacrawler-api .

# æ¨é€åˆ° Google Container Registry
docker push gcr.io/YOUR_PROJECT_ID/mediacrawler-api
```

2. **éƒ¨ç½²åˆ° Cloud Run**
```bash
gcloud run deploy mediacrawler-api \
  --image gcr.io/YOUR_PROJECT_ID/mediacrawler-api \
  --platform managed \
  --region us-central1 \
  --allow-unauthenticated \
  --memory 2Gi \
  --cpu 2 \
  --min-instances 1 \
  --max-instances 10 \
  --set-env-vars DATABASE_URL="postgresql://...",SUPABASE_URL="..."
```

### Azure Container Instances éƒ¨ç½²

```bash
az container create \
  --resource-group myResourceGroup \
  --name mediacrawler-api \
  --image your-registry.azurecr.io/mediacrawler-api:latest \
  --cpu 2 \
  --memory 4 \
  --port 8000 \
  --environment-variables DATABASE_URL="postgresql://..." \
  --restart-policy Always
```

## ğŸ”§ é…ç½®ä¼˜åŒ–

### ç”Ÿäº§ç¯å¢ƒé…ç½®è°ƒä¼˜

1. **Uvicorn é…ç½®**
```bash
# å¯åŠ¨å‘½ä»¤ä¼˜åŒ–
uvicorn app.main:app \
  --host 0.0.0.0 \
  --port 8000 \
  --workers 4 \
  --worker-class uvicorn.workers.UvicornWorker \
  --access-log \
  --log-level info \
  --no-server-header
```

2. **ç¯å¢ƒå˜é‡ä¼˜åŒ–**
```bash
# ç”Ÿäº§ç¯å¢ƒé…ç½®
DEBUG=false
LOG_LEVEL=WARNING
DEFAULT_HEADLESS=true
DEFAULT_ENABLE_PROXY=true
DEFAULT_MAX_RETRIES=3
DEFAULT_TIMEOUT=45

# æ•°æ®åº“è¿æ¥æ± 
DATABASE_POOL_SIZE=20
DATABASE_MAX_OVERFLOW=10

# ç¼“å­˜é…ç½®
REDIS_URL=redis://localhost:6379/0
CACHE_TTL=3600
```

### æ€§èƒ½ä¼˜åŒ–

1. **æ•°æ®åº“ä¼˜åŒ–**
```sql
-- åˆ›å»ºç´¢å¼•
CREATE INDEX idx_content_platform_created ON contents(platform, created_at);
CREATE INDEX idx_content_task_id ON contents(task_id);
CREATE INDEX idx_comments_content_id ON comments(content_id);

-- åˆ†åŒºè¡¨ï¼ˆå¤§æ•°æ®é‡æ—¶ï¼‰
CREATE TABLE contents_2024 PARTITION OF contents
FOR VALUES FROM ('2024-01-01') TO ('2025-01-01');
```

2. **ç¼“å­˜ç­–ç•¥**
```python
# Redis ç¼“å­˜é…ç½®
CACHE_CONFIG = {
    'task_results': 3600,      # ä»»åŠ¡ç»“æœç¼“å­˜1å°æ—¶
    'platform_stats': 1800,   # å¹³å°ç»Ÿè®¡ç¼“å­˜30åˆ†é’Ÿ
    'content_list': 600,       # å†…å®¹åˆ—è¡¨ç¼“å­˜10åˆ†é’Ÿ
}
```

## ğŸ” å®‰å…¨é…ç½®

### SSL/TLS é…ç½®

1. **Let's Encrypt è¯ä¹¦**
```bash
# å®‰è£… Certbot
sudo apt install certbot python3-certbot-nginx

# è·å–è¯ä¹¦
sudo certbot --nginx -d your-domain.com

# è‡ªåŠ¨ç»­æœŸ
sudo crontab -e
0 12 * * * /usr/bin/certbot renew --quiet
```

### é˜²ç«å¢™é…ç½®

```bash
# UFW é…ç½®
sudo ufw allow ssh
sudo ufw allow 80/tcp
sudo ufw allow 443/tcp
sudo ufw --force enable

# é™åˆ¶ API è®¿é—®é¢‘ç‡
sudo ufw limit 80/tcp
sudo ufw limit 443/tcp
```

### API å®‰å…¨

1. **é€Ÿç‡é™åˆ¶**
```python
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

@limiter.limit("100/minute")
@app.post("/api/v1/tasks")
async def create_task(request: Request, ...):
    ...
```

2. **CORS é…ç½®**
```python
from fastapi.middleware.cors import CORSMiddleware

app.add_middleware(
    CORSMiddleware,
    allow_origins=["https://your-frontend.com"],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE"],
    allow_headers=["*"],
)
```

## ğŸ”„ å¤‡ä»½å’Œæ¢å¤

### æ•°æ®å¤‡ä»½è„šæœ¬

```bash
#!/bin/bash
# backup.sh

BACKUP_DIR="/opt/backups/mediacrawler"
DATE=$(date +%Y%m%d_%H%M%S)
APP_DIR="/opt/mediacrawler-api"

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p $BACKUP_DIR

# å¤‡ä»½æ•°æ®æ–‡ä»¶
tar -czf $BACKUP_DIR/data_$DATE.tar.gz -C $APP_DIR data/

# å¤‡ä»½æ•°æ®åº“ (å¦‚æœä½¿ç”¨ PostgreSQL)
pg_dump mediacrawler > $BACKUP_DIR/database_$DATE.sql

# å‹ç¼©æ•°æ®åº“å¤‡ä»½
gzip $BACKUP_DIR/database_$DATE.sql

# æ¸…ç†æ—§å¤‡ä»½ (ä¿ç•™30å¤©)
find $BACKUP_DIR -name "*.tar.gz" -mtime +30 -delete
find $BACKUP_DIR -name "*.sql.gz" -mtime +30 -delete

echo "Backup completed: $DATE"
```

### æ¢å¤è„šæœ¬

```bash
#!/bin/bash
# restore.sh

BACKUP_FILE=$1
if [ -z "$BACKUP_FILE" ]; then
    echo "Usage: $0 <backup_file>"
    exit 1
fi

APP_DIR="/opt/mediacrawler-api"

# åœæ­¢æœåŠ¡
sudo systemctl stop mediacrawler-api

# æ¢å¤æ•°æ®æ–‡ä»¶
tar -xzf $BACKUP_FILE -C $APP_DIR

# æ¢å¤æ•°æ®åº“ (å¦‚æœæœ‰)
if [ -f "$2" ]; then
    gunzip -c $2 | psql mediacrawler
fi

# é‡å¯æœåŠ¡
sudo systemctl start mediacrawler-api

echo "Restore completed"
```

## ğŸ“Š ç›‘æ§å’Œå‘Šè­¦

### Prometheus ç›‘æ§

1. **å®‰è£… Prometheus å®¢æˆ·ç«¯**
```bash
pip install prometheus-client
```

2. **æ·»åŠ ç›‘æ§æŒ‡æ ‡**
```python
from prometheus_client import Counter, Histogram, Gauge, generate_latest

# å®šä¹‰æŒ‡æ ‡
task_counter = Counter('mediacrawler_tasks_total', 'Total tasks', ['platform', 'status'])
request_duration = Histogram('mediacrawler_request_duration_seconds', 'Request duration')
active_tasks = Gauge('mediacrawler_active_tasks', 'Active tasks')

@app.get("/metrics")
async def metrics():
    return Response(generate_latest(), media_type="text/plain")
```

### Grafana ä»ªè¡¨æ¿

```json
{
  "dashboard": {
    "title": "MediaCrawler API Server",
    "panels": [
      {
        "title": "Request Rate",
        "targets": [
          {
            "expr": "rate(mediacrawler_requests_total[5m])"
          }
        ]
      },
      {
        "title": "Active Tasks",
        "targets": [
          {
            "expr": "mediacrawler_active_tasks"
          }
        ]
      }
    ]
  }
}
```

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **ç«¯å£å ç”¨**
```bash
# æŸ¥æ‰¾å ç”¨ç«¯å£çš„è¿›ç¨‹
lsof -i :8000
netstat -tulpn | grep 8000

# æ€æ­»è¿›ç¨‹
kill -9 <PID>
```

2. **å†…å­˜ä¸è¶³**
```bash
# æ£€æŸ¥å†…å­˜ä½¿ç”¨
free -h
ps aux --sort=-%mem | head

# å¢åŠ äº¤æ¢ç©ºé—´
sudo fallocate -l 2G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
```

3. **æµè§ˆå™¨é©±åŠ¨é—®é¢˜**
```bash
# æ£€æŸ¥ Chrome ç‰ˆæœ¬
google-chrome --version
chromium --version

# æ›´æ–°é©±åŠ¨
sudo apt update
sudo apt install chromium chromium-driver
```

4. **æƒé™é—®é¢˜**
```bash
# æ£€æŸ¥æ–‡ä»¶æƒé™
ls -la /opt/mediacrawler-api/

# ä¿®å¤æƒé™
sudo chown -R www-data:www-data /opt/mediacrawler-api/
sudo chmod -R 755 /opt/mediacrawler-api/
```

### æ—¥å¿—åˆ†æ

```bash
# æŸ¥çœ‹åº”ç”¨æ—¥å¿—
journalctl -u mediacrawler-api -f

# æŸ¥çœ‹é”™è¯¯æ—¥å¿—
tail -f /opt/mediacrawler-api/logs/errors.log

# åˆ†æè®¿é—®æ—¥å¿—
tail -f /var/log/nginx/mediacrawler_access.log | grep -E "50[0-9]|40[0-9]"
```

---

**æ³¨æ„**: è¯·æ ¹æ®å®é™…ç¯å¢ƒè°ƒæ•´é…ç½®å‚æ•°ï¼Œç¡®ä¿å®‰å…¨æ€§å’Œæ€§èƒ½æ»¡è¶³éœ€æ±‚ã€‚ 