# MediaCrawler API Server

<div align="center">

![MediaCrawler API Server](frame.png)

**åŸºäº FastAPI çš„å¤šå¹³å°ç¤¾äº¤åª’ä½“æ•°æ®é‡‡é›† API æœåŠ¡**

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.100+-green.svg)](https://fastapi.tiangolo.com/)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)

</div>

## ğŸ“– é¡¹ç›®ç®€ä»‹

MediaCrawler API Server æ˜¯ä¸€ä¸ªåŸºäº FastAPI æ¡†æ¶æ„å»ºçš„é«˜æ€§èƒ½ç¤¾äº¤åª’ä½“æ•°æ®é‡‡é›†æœåŠ¡ï¼Œé€šè¿‡é€‚é…å™¨æ¨¡å¼å¤ç”¨åŸæœ‰çš„ MediaCrawler çˆ¬è™«åŠŸèƒ½ï¼Œä¸ºå¼€å‘è€…æä¾›ç»Ÿä¸€çš„ RESTful API æ¥å£æ¥é‡‡é›†å’Œç®¡ç†å¤šå¹³å°ç¤¾äº¤åª’ä½“æ•°æ®ã€‚

### ğŸ¯ æ ¸å¿ƒç‰¹æ€§

- ğŸš€ **å¤šå¹³å°æ”¯æŒ**: æ”¯æŒå°çº¢ä¹¦ã€æŠ–éŸ³ã€å¿«æ‰‹ã€Bç«™ã€å¾®åšã€ç™¾åº¦è´´å§ã€çŸ¥ä¹ç­‰7å¤§ä¸»æµå¹³å°
- ğŸ”§ **ç»Ÿä¸€æ¥å£**: æä¾›æ ‡å‡†åŒ–çš„ RESTful APIï¼Œç®€åŒ–å¤šå¹³å°æ•°æ®é‡‡é›†
- ğŸ›¡ï¸ **ç±»å‹å®‰å…¨**: åŸºäº Pydantic æ¨¡å‹çš„å®Œæ•´ç±»å‹æ£€æŸ¥å’Œé…ç½®éªŒè¯
- ğŸ“Š **å¤šå­˜å‚¨æ”¯æŒ**: æ”¯æŒ JSONã€CSVã€Supabase ç­‰å¤šç§æ•°æ®å­˜å‚¨æ–¹å¼
- ğŸ”„ **å¼‚æ­¥å¤„ç†**: åŸºäº FastAPI çš„é«˜æ€§èƒ½å¼‚æ­¥å¤„ç†æ¶æ„
- ğŸ›ï¸ **çµæ´»é…ç½®**: æ”¯æŒå¤šå±‚çº§é…ç½®ç®¡ç†ï¼Œæ»¡è¶³ä¸åŒåœºæ™¯éœ€æ±‚
- ğŸ“ˆ **å®æ—¶ç›‘æ§**: æä¾›ä»»åŠ¡çŠ¶æ€ç›‘æ§ã€è¿›åº¦è·Ÿè¸ªå’Œæ—¥å¿—ç®¡ç†

## ğŸ—ï¸ è®¾è®¡æ€æƒ³

### æ ¸å¿ƒæ¶æ„åŸåˆ™

#### 1. **é€‚é…å™¨æ¨¡å¼ (Adapter Pattern)**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI API   â”‚ => â”‚  Crawler Adapter â”‚ => â”‚  MediaCrawler Core  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

é€šè¿‡é€‚é…å™¨æ¨¡å¼å°è£…åŸæœ‰çš„ MediaCrawler åŠŸèƒ½ï¼Œæä¾›ç»Ÿä¸€çš„ API æ¥å£ï¼Œå®ç°æ–°æ—§ç³»ç»Ÿçš„æ— ç¼é›†æˆã€‚

#### 2. **é…ç½®ç»Ÿä¸€æ”¶å£**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ConfigManager (ç»Ÿä¸€é…ç½®ç®¡ç†)                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   AppConfig     â”‚  CrawlerConfig  â”‚     StorageConfig       â”‚
â”‚   (åº”ç”¨çº§é…ç½®)    â”‚  (çˆ¬è™«ä»»åŠ¡é…ç½®)   â”‚     (æ•°æ®å­˜å‚¨é…ç½®)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

æ‰€æœ‰é…ç½®ç»Ÿä¸€é€šè¿‡ ConfigManager ç®¡ç†ï¼ŒåŸºäº Pydantic æ¨¡å‹æä¾›ç±»å‹å®‰å…¨çš„é…ç½®äº¤äº’ã€‚

#### 3. **æ¨¡å—åŒ–åˆ†å±‚æ¶æ„**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        API Layer                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                      Business Layer                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Crawler Adapter â”‚  â”‚  Data Reader    â”‚  â”‚ Login Managerâ”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                        Core Layer                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Config Manager  â”‚  â”‚ Logging Manager â”‚  â”‚Database Utilsâ”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                   Integration Layer                        â”‚
â”‚                  MediaCrawler Core                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ—ï¸ ä»£ç æ¡†æ¶

### ç›®å½•ç»“æ„
```
MediaCrawler-ApiServer/
â”œâ”€â”€ app/                          # åº”ç”¨ä¸»ç›®å½•
â”‚   â”œâ”€â”€ api/                      # APIè·¯ç”±å±‚
â”‚   â”‚   â”œâ”€â”€ data.py              # æ•°æ®æŸ¥è¯¢API
â”‚   â”‚   â””â”€â”€ login.py             # ç™»å½•ç®¡ç†API
â”‚   â”œâ”€â”€ core/                     # æ ¸å¿ƒæ¨¡å—
â”‚   â”‚   â”œâ”€â”€ config_manager.py    # ç»Ÿä¸€é…ç½®ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ config.py            # åŸºç¡€é…ç½®
â”‚   â”‚   â”œâ”€â”€ logging.py           # æ—¥å¿—ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ database.py          # æ•°æ®åº“å·¥å…·
â”‚   â”‚   â””â”€â”€ login_manager.py     # ç™»å½•çŠ¶æ€ç®¡ç†
â”‚   â”œâ”€â”€ crawler/                  # çˆ¬è™«é€‚é…å™¨
â”‚   â”‚   â”œâ”€â”€ adapter.py           # ä¸»é€‚é…å™¨
â”‚   â”‚   â””â”€â”€ core/                # çˆ¬è™«æ ¸å¿ƒç»„ä»¶
â”‚   â”œâ”€â”€ dataReader/              # æ•°æ®è¯»å–å™¨
â”‚   â”‚   â”œâ”€â”€ base.py              # åŸºç¡€æŠ½è±¡ç±»
â”‚   â”‚   â”œâ”€â”€ factory.py           # å·¥å‚æ¨¡å¼
â”‚   â”‚   â”œâ”€â”€ json_reader.py       # JSONæ•°æ®è¯»å–
â”‚   â”‚   â”œâ”€â”€ csv_reader.py        # CSVæ•°æ®è¯»å–
â”‚   â”‚   â””â”€â”€ supabase_reader.py   # Supabaseæ•°æ®è¯»å–
â”‚   â”œâ”€â”€ models/                   # æ•°æ®æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ base.py              # åŸºç¡€æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ comment.py           # è¯„è®ºæ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ content.py           # å†…å®¹æ¨¡å‹
â”‚   â”‚   â””â”€â”€ task.py              # ä»»åŠ¡æ¨¡å‹
â”‚   â””â”€â”€ main.py                   # FastAPIåº”ç”¨å…¥å£
â”œâ”€â”€ MediaCrawler/                 # åŸMediaCrawleré¡¹ç›®
â”œâ”€â”€ data/                         # æ•°æ®å­˜å‚¨ç›®å½•
â”œâ”€â”€ logs/                         # æ—¥å¿—æ–‡ä»¶
â”œâ”€â”€ tests/                        # æµ‹è¯•æ–‡ä»¶
â”œâ”€â”€ examples/                     # ç¤ºä¾‹ä»£ç 
â””â”€â”€ config.env.example           # é…ç½®æ–‡ä»¶æ¨¡æ¿
```

### æ ¸å¿ƒæ¨¡å—è¯´æ˜

#### 1. **é…ç½®ç®¡ç†æ¨¡å—** (`app/core/config_manager.py`)
```python
# åº”ç”¨çº§é…ç½®
class AppConfig(BaseModel):
    app_name: str = "MediaCrawler API Server"
    version: str = "1.0.0"
    supported_platforms: List[str]

# çˆ¬è™«é…ç½®è¯·æ±‚ï¼ˆAPIå±‚ï¼‰
class CrawlerConfigRequest(BaseModel):
    enable_proxy: Optional[bool] = None
    headless: Optional[bool] = None
    max_retries: Optional[int] = None

# å®Œæ•´çˆ¬è™«é…ç½®ï¼ˆå†…éƒ¨ä½¿ç”¨ï¼‰
class CrawlerConfig(BaseModel):
    platform: str
    enable_proxy: bool = False
    headless: bool = True
    max_retries: int = 3

# å­˜å‚¨é…ç½®
class StorageConfig(BaseModel):
    source_type: str
    connection_timeout: int = 30
    retry_times: int = 3
```

#### 2. **çˆ¬è™«é€‚é…å™¨** (`app/crawler/adapter.py`)
```python
class MediaCrawlerAdapter:
    async def start_crawler_task(self, task: CrawlerTask)
    async def get_task_status(self, task_id: str)
    async def get_task_result(self, task_id: str)
    async def stop_task(self, task_id: str)
```

#### 3. **æ•°æ®è¯»å–å™¨** (`app/dataReader/`)
```python
class BaseDataReader(ABC):
    @abstractmethod
    async def get_content_list(self, platform, filters)
    @abstractmethod
    async def get_content_by_id(self, platform, content_id)
    @abstractmethod
    async def search_content(self, platform, keyword, filters)
```

## ğŸ”„ æ¨¡å—æµç¨‹

### 1. çˆ¬è™«ä»»åŠ¡æ‰§è¡Œæµç¨‹
```
Client Request â†’ FastAPI â†’ ConfigManager â†’ CrawlerAdapter â†’ MediaCrawler
                                â†“
Response â† JSON Result â† Task Result â† Crawler Execution â† Core Engine
```

### 2. é…ç½®ç®¡ç†æµç¨‹
```
API Request â†’ CrawlerConfigRequest â†’ ConfigManager.build_crawler_config()
                                           â†“
é»˜è®¤é…ç½® + å¹³å°é…ç½® + ç¯å¢ƒå˜é‡ + APIé…ç½® â†’ CrawlerConfig â†’ ç±»å‹éªŒè¯ â†’ æœ€ç»ˆé…ç½®
```

### 3. æ•°æ®æŸ¥è¯¢æµç¨‹
```
Data API Request â†’ DataReaderFactory â†’ DataReader â†’ DataSource
                                          â†“
JSON Response â† Formatted Result â† Query Result â† Raw Data
```

## ğŸ“¡ æ¥å£è§„èŒƒ

### 1. çˆ¬è™«ä»»åŠ¡ç®¡ç†

#### åˆ›å»ºçˆ¬è™«ä»»åŠ¡
```http
POST /api/v1/tasks
Content-Type: application/json

{
  "platform": "xhs",
  "task_type": "search",
  "keywords": ["ç¾é£Ÿ", "æ—…è¡Œ"],
  "max_count": 100,
  "max_comments": 50,
  "headless": true,
  "enable_proxy": false,
  "save_data_option": "db",
  "config": {
    "enable_proxy": false,
    "headless": true,
    "max_retries": 3,
    "timeout": 30
  }
}
```

**å“åº”:**
```json
{
  "task_id": "uuid-string",
  "message": "ä»»åŠ¡å·²åˆ›å»ºå¹¶å¼€å§‹æ‰§è¡Œ"
}
```

#### æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€
```http
GET /api/v1/tasks/{task_id}/status
```

**å“åº”:**
```json
{
  "task_id": "uuid-string",
  "status": "running",
  "done": false,
  "progress": {
    "current_stage": "æ•°æ®é‡‡é›†ä¸­",
    "progress_percent": 45.6,
    "items_completed": 45,
    "items_total": 100,
    "items_failed": 2
  }
}
```

#### è·å–ä»»åŠ¡ç»“æœ
```http
GET /api/v1/tasks/{task_id}/result
```

**å“åº”:**
```json
{
  "task_id": "uuid-string",
  "success": true,
  "message": "ä»»åŠ¡æ‰§è¡ŒæˆåŠŸ",
  "data_count": 98,
  "error_count": 2,
  "data": [
    {
      "note_id": "67e6c0c30000000009016264",
      "title": "ç¾é£Ÿåˆ†äº«",
      "author": "ç”¨æˆ·å",
      "liked_count": 1234,
      "comments_count": 56,
      "publish_time": "2024-01-01 12:00:00"
    }
  ]
}
```

### 2. æ•°æ®æŸ¥è¯¢æ¥å£

#### è·å–å†…å®¹åˆ—è¡¨
```http
GET /api/v1/data/content/{platform}?source_type=json&limit=20&offset=0
```

#### è·å–å†…å®¹è¯¦æƒ…
```http
GET /api/v1/data/content/{platform}/{content_id}?source_type=json
```

#### æœç´¢å†…å®¹
```http
GET /api/v1/data/search/{platform}?keyword=ç¾é£Ÿ&limit=20
```

#### è·å–ç”¨æˆ·å†…å®¹
```http
GET /api/v1/data/user/{platform}/{user_id}/content?limit=20
```

### 3. ç™»å½•ç®¡ç†æ¥å£

#### åˆ›å»ºç™»å½•ä¼šè¯
```http
POST /api/v1/login/create-session
{
  "task_id": "uuid-string",
  "platform": "xhs",
  "login_type": "qrcode",
  "timeout": 300
}
```

#### è·å–ç™»å½•çŠ¶æ€
```http
GET /api/v1/login/status/{task_id}
```

### 4. ç³»ç»Ÿç®¡ç†æ¥å£

#### å¥åº·æ£€æŸ¥
```http
GET /api/v1/data/health
```

#### è·å–æ”¯æŒçš„å¹³å°
```http
GET /api/v1/data/platforms
```

#### è·å–é…ç½®é€‰é¡¹
```http
GET /api/v1/system/config/options
```

## ğŸ“š ä½¿ç”¨æ‰‹å†Œ

### 1. ç¯å¢ƒæ­å»º

#### ç³»ç»Ÿè¦æ±‚
- Python 3.8+
- å†…å­˜: 4GB+
- å­˜å‚¨: 10GB+

#### å®‰è£…æ­¥éª¤

1. **å…‹éš†é¡¹ç›®**
```bash
git clone https://github.com/your-repo/MediaCrawler-ApiServer.git
cd MediaCrawler-ApiServer
```

2. **åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ**
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# æˆ–
venv\Scripts\activate     # Windows
```

3. **å®‰è£…ä¾èµ–**
```bash
pip install -r requirements.txt
```

4. **é…ç½®ç¯å¢ƒå˜é‡**
```bash
cp config.env.example .env
# ç¼–è¾‘ .env æ–‡ä»¶ï¼Œé…ç½®å¿…è¦çš„ç¯å¢ƒå˜é‡
```

5. **å¯åŠ¨æœåŠ¡**
```bash
python -m uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

### 2. é…ç½®è¯´æ˜

#### ç¯å¢ƒå˜é‡é…ç½® (`.env`)
```bash
# æ•°æ®åº“é…ç½®
DATABASE_URL=sqlite:///./data/app.db
SUPABASE_URL=your_supabase_url
SUPABASE_KEY=your_supabase_key

# ä»£ç†é…ç½®
DEFAULT_ENABLE_PROXY=false
DEFAULT_PROXY_PROVIDER=kuaidaili

# çˆ¬è™«é…ç½®
DEFAULT_HEADLESS=true
DEFAULT_MAX_RETRIES=3
DEFAULT_TIMEOUT=30

# æ—¥å¿—é…ç½®
LOG_LEVEL=INFO
LOG_FILE=logs/app.log
```

#### å¹³å°é…ç½®
æ¯ä¸ªå¹³å°éƒ½æœ‰é»˜è®¤çš„é…ç½®å‚æ•°ï¼Œå¯ä»¥é€šè¿‡ API è¯·æ±‚è¿›è¡Œè¦†ç›–ï¼š

```python
# å°çº¢ä¹¦å¹³å°é»˜è®¤é…ç½®
{
    "delay_range": [2, 4],
    "max_comments": 100,
    "timeout": 45
}

# æŠ–éŸ³å¹³å°é»˜è®¤é…ç½®  
{
    "delay_range": [1, 2],
    "max_comments": 50,
    "timeout": 30
}
```

### 3. å¿«é€Ÿå¼€å§‹

#### ç¤ºä¾‹1: å°çº¢ä¹¦å…³é”®è¯æœç´¢
```python
import requests

# åˆ›å»ºæœç´¢ä»»åŠ¡
task_data = {
    "platform": "xhs",
    "task_type": "search", 
    "keywords": ["ç¾é£Ÿæ¨è"],
    "max_count": 50,
    "max_comments": 20,
    "headless": True,
    "save_data_option": "json"
}

response = requests.post("http://localhost:8000/api/v1/tasks", json=task_data)
task_id = response.json()["task_id"]

# ç›‘æ§ä»»åŠ¡çŠ¶æ€
import time
while True:
    status = requests.get(f"http://localhost:8000/api/v1/tasks/{task_id}/status")
    if status.json()["done"]:
        break
    time.sleep(3)

# è·å–ç»“æœ
result = requests.get(f"http://localhost:8000/api/v1/tasks/{task_id}/result")
print(result.json())
```

#### ç¤ºä¾‹2: æŸ¥è¯¢å·²æœ‰æ•°æ®
```python
import requests

# æŸ¥è¯¢å°çº¢ä¹¦å†…å®¹åˆ—è¡¨
response = requests.get(
    "http://localhost:8000/api/v1/data/content/xhs",
    params={"source_type": "json", "limit": 20}
)

data = response.json()
print(f"æ‰¾åˆ° {data['total']} æ¡æ•°æ®")
for item in data['data']:
    print(f"- {item['title']} (ç‚¹èµ: {item['liked_count']})")
```

#### ç¤ºä¾‹3: ä½¿ç”¨è‡ªå®šä¹‰é…ç½®
```python
import requests

# ä½¿ç”¨ä»£ç†å’Œè‡ªå®šä¹‰é…ç½®
task_data = {
    "platform": "douyin",
    "task_type": "search",
    "keywords": ["èˆè¹ˆ"],
    "max_count": 30,
    "config": {
        "enable_proxy": True,
        "proxy_provider": "kuaidaili", 
        "headless": False,
        "max_retries": 5,
        "timeout": 60,
        "delay_range": [3, 6]
    }
}

response = requests.post("http://localhost:8000/api/v1/tasks", json=task_data)
```

### 4. éƒ¨ç½²æŒ‡å—

#### Docker éƒ¨ç½²
```dockerfile
FROM python:3.9-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .
EXPOSE 8000

CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

```bash
# æ„å»ºé•œåƒ
docker build -t mediacrawler-api .

# è¿è¡Œå®¹å™¨
docker run -d -p 8000:8000 \
  -v $(pwd)/data:/app/data \
  -v $(pwd)/logs:/app/logs \
  --env-file .env \
  mediacrawler-api
```

#### Nginx é…ç½®
```nginx
server {
    listen 80;
    server_name your-domain.com;

    location / {
        proxy_pass http://127.0.0.1:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    }
}
```

### 5. æ€§èƒ½ä¼˜åŒ–

#### å¹¶å‘æ§åˆ¶
```python
# åœ¨é…ç½®ä¸­è®¾ç½®åˆç†çš„å¹¶å‘å‚æ•°
config = {
    "max_retries": 3,           # é‡è¯•æ¬¡æ•°
    "timeout": 30,              # è¶…æ—¶æ—¶é—´
    "delay_range": [2, 4],      # è¯·æ±‚é—´éš”
    "enable_proxy": True,       # ä½¿ç”¨ä»£ç†æ± 
    "batch_size": 100           # æ‰¹å¤„ç†å¤§å°
}
```

#### å†…å­˜ç®¡ç†
- åˆç†è®¾ç½® `max_count` å‚æ•°ï¼Œé¿å…ä¸€æ¬¡æ€§é‡‡é›†è¿‡å¤šæ•°æ®
- å®šæœŸæ¸…ç†å®Œæˆçš„ä»»åŠ¡ç»“æœ
- ä½¿ç”¨æµå¼å¤„ç†å¤§å‹æ•°æ®é›†

#### å­˜å‚¨ä¼˜åŒ–
- JSON: é€‚åˆå°è§„æ¨¡æ•°æ®å’Œå¿«é€ŸæŸ¥è¯¢
- CSV: é€‚åˆå¤§è§„æ¨¡æ•°æ®å’Œæ•°æ®åˆ†æ
- Supabase: é€‚åˆç”Ÿäº§ç¯å¢ƒå’Œå¤šç”¨æˆ·åœºæ™¯

## ğŸ”§ å…¶ä»–

### æ•…éšœæ’é™¤

#### å¸¸è§é—®é¢˜

1. **ç«¯å£å ç”¨**
```bash
# æ£€æŸ¥ç«¯å£å ç”¨
lsof -i :8000
# æ€æ­»è¿›ç¨‹
kill -9 <pid>
```

2. **ä¾èµ–å†²çª**
```bash
# é‡æ–°åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
rm -rf venv
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

3. **æƒé™é—®é¢˜**
```bash
# ç¡®ä¿æ•°æ®ç›®å½•æœ‰å†™æƒé™
chmod 755 data logs
```

4. **æµè§ˆå™¨é©±åŠ¨é—®é¢˜**
```bash
# å®‰è£…Chromeé©±åŠ¨
apt-get update
apt-get install -y chromium-browser chromium-chromedriver
```

### ç›‘æ§å’Œæ—¥å¿—

#### æ—¥å¿—çº§åˆ«
- `DEBUG`: è¯¦ç»†è°ƒè¯•ä¿¡æ¯
- `INFO`: ä¸€èˆ¬ä¿¡æ¯ (é»˜è®¤)
- `WARNING`: è­¦å‘Šä¿¡æ¯
- `ERROR`: é”™è¯¯ä¿¡æ¯

#### æ—¥å¿—æ–‡ä»¶
```
logs/
â”œâ”€â”€ app.log          # åº”ç”¨ä¸»æ—¥å¿—
â”œâ”€â”€ errors.log       # é”™è¯¯æ—¥å¿—  
â”œâ”€â”€ access.log       # è®¿é—®æ—¥å¿—
â””â”€â”€ crawler.log      # çˆ¬è™«æ‰§è¡Œæ—¥å¿—
```

#### ç›‘æ§æŒ‡æ ‡
- ä»»åŠ¡æˆåŠŸç‡
- å¹³å‡å“åº”æ—¶é—´
- æ•°æ®é‡‡é›†é‡
- é”™è¯¯ç‡ç»Ÿè®¡

### æ‰©å±•å¼€å‘

#### æ·»åŠ æ–°å¹³å°æ”¯æŒ
1. åœ¨ `MediaCrawler` ä¸­å®ç°å¹³å°çˆ¬è™«
2. åœ¨ `PlatformType` æšä¸¾ä¸­æ·»åŠ æ–°å¹³å°
3. åœ¨ `ConfigManager` ä¸­é…ç½®å¹³å°å‚æ•°
4. ç¼–å†™å¯¹åº”çš„æµ‹è¯•ç”¨ä¾‹

#### è‡ªå®šä¹‰æ•°æ®è¯»å–å™¨
```python
class CustomDataReader(BaseDataReader):
    async def get_content_list(self, platform, filters):
        # å®ç°è‡ªå®šä¹‰è¯»å–é€»è¾‘
        pass
```

#### æ·»åŠ æ–°çš„å­˜å‚¨æ–¹å¼
```python
class CustomStorageConfig(BaseModel):
    # å®šä¹‰å­˜å‚¨é…ç½®
    pass

# åœ¨ ConfigManager ä¸­æ³¨å†Œ
def build_storage_config(self, source_type: str, platform: str = None):
    if source_type == "custom":
        return CustomStorageConfig(...)
```

### ç¤¾åŒºè´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

1. Fork é¡¹ç›®
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯: `git checkout -b feature/new-feature`
3. æäº¤æ›´æ”¹: `git commit -am 'Add new feature'`
4. æ¨é€åˆ†æ”¯: `git push origin feature/new-feature`
5. æäº¤ Pull Request

### è®¸å¯è¯

æœ¬é¡¹ç›®åŸºäº MIT è®¸å¯è¯å¼€æºï¼Œè¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

### è”ç³»æ–¹å¼

- é¡¹ç›®ä¸»é¡µ: https://github.com/your-repo/MediaCrawler-ApiServer
- é—®é¢˜åé¦ˆ: https://github.com/your-repo/MediaCrawler-ApiServer/issues
- è®¨è®ºç¤¾åŒº: https://github.com/your-repo/MediaCrawler-ApiServer/discussions

---

<div align="center">

**å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸ª â­ Star æ”¯æŒä¸€ä¸‹ï¼**

</div> 