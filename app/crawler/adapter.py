"""
MediaCrawler é€‚é…å™¨

é€šè¿‡é€‚é…å™¨æ¨¡å¼å¤ç”¨åŸMediaCrawleré¡¹ç›®çš„æˆç†Ÿçˆ¬è™«åŠŸèƒ½ï¼Œ
ä¸ºFastAPIæœåŠ¡æä¾›ç»Ÿä¸€çš„çˆ¬è™«æ¥å£ã€‚
"""

import asyncio
import sys
import os
import subprocess
import tempfile
import json
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from enum import Enum
import logging
from pathlib import Path

from app.dataReader.base import PlatformType
from app.core.logging import logging_manager, TaskEventType, get_app_logger
from app.core.config_manager import get_config_manager, CrawlerConfigRequest, CrawlerConfig
from app.core.login_manager import login_manager, LoginType, LoginStatus
from app.core.config import get_settings
from app.core.cookies_manager import cookies_manager


logger = get_app_logger(__name__)

# åŸMediaCrawleré¡¹ç›®è·¯å¾„
MEDIACRAWLER_PATH = os.path.join(os.path.dirname(__file__), "..", "..", "MediaCrawler")


class CrawlerTaskType(Enum):
    """çˆ¬è™«ä»»åŠ¡ç±»å‹"""
    SEARCH = "search"
    DETAIL = "detail" 
    CREATOR = "creator"


@dataclass
class CrawlerTask:
    """çˆ¬è™«ä»»åŠ¡"""
    task_id: str
    platform: PlatformType
    task_type: CrawlerTaskType
    keywords: Optional[List[str]] = None
    content_ids: Optional[List[str]] = None
    creator_ids: Optional[List[str]] = None
    max_count: int = 100
    max_comments: int = 50
    start_page: int = 1
    enable_proxy: bool = False
    headless: bool = True
    enable_comments: bool = True
    enable_sub_comments: bool = False
    save_data_option: str = "db"  # db, json, csv
    config: Optional[CrawlerConfigRequest] = None  # è‡ªå®šä¹‰é…ç½®
    clear_cookies: bool = False  # æ˜¯å¦æ¸…é™¤cookiesé‡æ–°ç™»å½•


@dataclass 
class CrawlerResult:
    """çˆ¬è™«ç»“æœ"""
    task_id: str
    success: bool
    message: str
    data_count: int = 0
    error_count: int = 0
    data: Optional[List[Dict]] = None
    errors: Optional[List[str]] = None


class MediaCrawlerAdapter:
    """MediaCrawler é€‚é…å™¨ - é€šè¿‡è¿›ç¨‹è°ƒç”¨å¤ç”¨åŸé¡¹ç›®åŠŸèƒ½"""
    
    def __init__(self):
        self.running_tasks: Dict[str, asyncio.Task] = {}
        self.task_results: Dict[str, CrawlerResult] = {}
        
    async def start_crawler_task(self, task: CrawlerTask) -> str:
        """å¯åŠ¨çˆ¬è™«ä»»åŠ¡"""
        
        # åˆ›å»ºä»»åŠ¡æ—¥å¿—è®°å½•å™¨
        platform_str = self._get_platform_string(task.platform)
        task_logger = logging_manager.create_task_logger(task.task_id, platform_str)
        
        try:
            task_logger.log_event(
                TaskEventType.TASK_STARTED,
                f"å¼€å§‹å¯åŠ¨çˆ¬è™«ä»»åŠ¡: å¹³å°={platform_str}, ç±»å‹={task.task_type.value}",
                data={
                    "platform": platform_str,
                    "task_type": task.task_type.value,
                    "keywords": task.keywords,
                    "content_ids": task.content_ids,
                    "creator_ids": task.creator_ids,
                    "max_count": task.max_count,
                    "max_comments": task.max_comments
                }
            )
            
            # å¯åŠ¨å¼‚æ­¥ä»»åŠ¡
            async_task = asyncio.create_task(
                self._run_mediacrawler_process(task, task_logger)
            )
            
            self.running_tasks[task.task_id] = async_task
            
            logger.info(f"çˆ¬è™«ä»»åŠ¡ {task.task_id} å·²å¯åŠ¨")
            return task.task_id
            
        except Exception as e:
            error_msg = f"å¯åŠ¨çˆ¬è™«ä»»åŠ¡å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            
            task_logger.log_event(
                TaskEventType.TASK_FAILED,
                error_msg,
                error=str(e)
            )
            
            self.task_results[task.task_id] = CrawlerResult(
                task_id=task.task_id,
                success=False,
                message=error_msg
            )
            raise
    
    async def _run_mediacrawler_process(self, task: CrawlerTask, task_logger):
        """è¿è¡ŒMediaCrawlerè¿›ç¨‹"""
        result = None
        try:
            task_logger.log_event(TaskEventType.TASK_PROGRESS, "å¼€å§‹å‡†å¤‡MediaCrawler")
            
            # 1. å‡†å¤‡é…ç½®ï¼ˆç°åœ¨åªæ˜¯æ—¥å¿—è®°å½•ï¼‰
            await self._create_temp_config(task)
            
            # 2. æ„å»ºæ‰§è¡Œå‘½ä»¤
            cmd = self._build_crawler_command(task)
            
            task_logger.log_event(TaskEventType.TASK_PROGRESS, "å¼€å§‹æ‰§è¡ŒMediaCrawler")
            
            # 3. æ‰§è¡Œçˆ¬è™«è¿›ç¨‹
            result = await self._execute_crawler_process(cmd, task_logger)
            
            task_logger.log_event(
                TaskEventType.TASK_COMPLETED if result["success"] else TaskEventType.TASK_FAILED,
                f"MediaCrawleræ‰§è¡Œ{'æˆåŠŸ' if result['success'] else 'å¤±è´¥'}: {result['message']}"
            )
            
            # 4. ä¿å­˜ä»»åŠ¡ç»“æœ
            self.task_results[task.task_id] = result
            
            # 5. æ›´æ–°ä»»åŠ¡çŠ¶æ€
            if task.task_id in self.running_tasks:
                self.running_tasks[task.task_id]['status'] = 'completed' if result['success'] else 'failed'
                self.running_tasks[task.task_id]['done'] = True
                self.running_tasks[task.task_id]['result'] = result
                
            return result
            
        except Exception as e:
            error_msg = f"æ‰§è¡ŒMediaCrawleræ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            task_logger.log_event(TaskEventType.TASK_FAILED, error_msg)
            
            result = {
                "success": False,
                "message": error_msg,
                "data_count": 0,
                "error_count": 1,
                "data": []
            }
            
            # ä¿å­˜é”™è¯¯ç»“æœ
            self.task_results[task.task_id] = result
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            if task.task_id in self.running_tasks:
                self.running_tasks[task.task_id]['status'] = 'failed'
                self.running_tasks[task.task_id]['done'] = True
                self.running_tasks[task.task_id]['result'] = result
                
            return result
    
    async def _create_temp_config(self, task: CrawlerTask) -> None:
        """
        å‡†å¤‡æœ€ç»ˆçš„é…ç½®å‚æ•°
        ä½¿ç”¨åŸºäºPydanticæ¨¡å‹çš„ç±»å‹å®‰å…¨é…ç½®ç®¡ç†
        """
        try:
            config_manager = get_config_manager()
            
            # ğŸ¯ ä½¿ç”¨æ–°çš„åŸºäºæ¨¡å‹çš„é…ç½®ç®¡ç†
            # æ„å»ºçˆ¬è™«é…ç½®ï¼Œä¼ å…¥ä»»åŠ¡çš„configä½œä¸ºrequest_config
            crawler_config: CrawlerConfig = config_manager.build_crawler_config(
                platform=task.platform.value,
                request_config=task.config  # è¿™æ˜¯CrawlerConfigRequestç±»å‹
            )
            
            # ä»å…¶ä»–ä»»åŠ¡å‚æ•°åˆ›å»ºè¦†ç›–é…ç½®
            task_overrides = CrawlerConfigRequest(
                enable_proxy=task.enable_proxy,
                headless=task.headless,
                enable_comments=task.enable_comments,
                enable_sub_comments=task.enable_sub_comments,
                max_comments=task.max_comments,
                save_data_option=task.save_data_option
            )
            
            # é‡æ–°æ„å»ºé…ç½®ï¼Œåº”ç”¨ä»»åŠ¡çº§è¦†ç›–
            final_config: CrawlerConfig = config_manager.build_crawler_config(
                platform=task.platform.value,
                request_config=task_overrides if not task.config else task.config
            )
            
            logger.info(f"ğŸ”§ é…ç½®æ„å»ºå®Œæˆ - å¹³å°: {task.platform.value}")
            logger.info(f"ğŸ“ æœ€ç»ˆé…ç½®: headless={final_config.headless}, enable_proxy={final_config.enable_proxy}, timeout={final_config.timeout}")
            
            # ä¿å­˜ç±»å‹å®‰å…¨çš„é…ç½®å¯¹è±¡
            task._final_config = final_config
            
        except Exception as e:
            logger.error(f"é…ç½®æ„å»ºå¤±è´¥: {e}")
            raise
    
    def _build_crawler_command(self, task: CrawlerTask) -> List[str]:
        """æ„å»ºMediaCrawlerå‘½ä»¤è¡Œ"""
        logger.info(f"ğŸ”§ å¼€å§‹æ„å»ºå‘½ä»¤è¡Œå‚æ•°...")
        
        platform_str = self._get_platform_string(task.platform)
        
        # åŸºç¡€å‘½ä»¤
        cmd = [
            "python", "main.py",
            "--platform", platform_str,
            "--type", task.task_type.value,
            "--max_count", str(task.max_count),
            "--max_comments", str(task.max_comments),
            "--headless", str(task.headless).lower(),
            "--enable_proxy", str(task.enable_proxy).lower(),
            "--save_data_option", task.save_data_option
        ]
        
        # ğŸª Cookieså¤„ç†é€»è¾‘
        if task.clear_cookies:
            # æ¸…é™¤cookiesï¼Œå¼ºåˆ¶é‡æ–°ç™»å½•
            cookies_manager.clear_cookies(platform_str)
            logger.info(f"ğŸ—‘ï¸  å·²æ¸…é™¤cookiesï¼Œå°†é‡æ–°ç™»å½•: {platform_str}")
        else:
            # å°è¯•åŠ è½½ç¼“å­˜çš„cookies
            cached_cookies = cookies_manager.load_cookies(platform_str, max_age_days=7)
            if cached_cookies:
                cmd.extend(["--cookies", cached_cookies])
                logger.info(f"ğŸª ä½¿ç”¨ç¼“å­˜çš„cookies: {platform_str}")
            else:
                logger.info(f"ğŸ“„ æœªæ‰¾åˆ°æœ‰æ•ˆcookiesï¼Œå°†éœ€è¦é‡æ–°ç™»å½•: {platform_str}")
        
        # æ ¹æ®ä»»åŠ¡ç±»å‹æ·»åŠ ç‰¹å®šå‚æ•°
        if task.task_type == CrawlerTaskType.SEARCH:
            if task.keywords:
                cmd.extend(["--keywords", ",".join(task.keywords)])
                logger.info(f"ğŸ” æœç´¢å…³é”®è¯: {','.join(task.keywords)}")
            else:
                logger.warning("âš ï¸  Searchæ¨¡å¼éœ€è¦æä¾›keywordså‚æ•°")
        
        elif task.task_type == CrawlerTaskType.DETAIL:
            if task.content_ids:
                # æ ¹æ®å¹³å°è®¾ç½®ç›¸åº”çš„å†…å®¹å‚æ•°
                if platform_str == "xhs":
                    cmd.extend(["--xhs_note_urls", ";".join(task.content_ids)])
                    logger.info(f"ğŸ”§ å°çº¢ä¹¦ç¬”è®°URLåˆ—è¡¨: {','.join(task.content_ids)}")
                
                elif platform_str == "dy":
                    cmd.extend(["--dy_ids", ";".join(task.content_ids)])
                    logger.info(f"ğŸ”§ æŠ–éŸ³è§†é¢‘IDåˆ—è¡¨: {','.join(task.content_ids)}")
                
                elif platform_str == "ks":
                    cmd.extend(["--ks_ids", ";".join(task.content_ids)])
                    logger.info(f"ğŸ”§ å¿«æ‰‹è§†é¢‘IDåˆ—è¡¨: {','.join(task.content_ids)}")
                
                elif platform_str == "bili":
                    cmd.extend(["--bili_ids", ";".join(task.content_ids)])
                    logger.info(f"ğŸ”§ Bç«™è§†é¢‘BVIDåˆ—è¡¨: {','.join(task.content_ids)}")
                
                elif platform_str == "wb":
                    cmd.extend(["--weibo_ids", ";".join(task.content_ids)])
                    logger.info(f"ğŸ”§ å¾®åšå¸–å­IDåˆ—è¡¨: {','.join(task.content_ids)}")
                
                elif platform_str == "zhihu":
                    cmd.extend(["--zhihu_urls", ";".join(task.content_ids)])
                    logger.info(f"ğŸ”§ çŸ¥ä¹URLåˆ—è¡¨: {','.join(task.content_ids)}")
            else:
                logger.warning("âš ï¸  Detailæ¨¡å¼éœ€è¦æä¾›content_idså‚æ•°")
        
        elif task.task_type == CrawlerTaskType.CREATOR:
            if task.creator_ids:
                # æ ¹æ®å¹³å°è®¾ç½®ç›¸åº”çš„åˆ›ä½œè€…å‚æ•°
                if platform_str == "xhs":
                    cmd.extend(["--xhs_creator_ids", ";".join(task.creator_ids)])
                    logger.info(f"ğŸ”§ å°çº¢ä¹¦åˆ›ä½œè€…IDåˆ—è¡¨: {','.join(task.creator_ids)}")
                
                elif platform_str == "dy":
                    cmd.extend(["--dy_creator_ids", ";".join(task.creator_ids)])
                    logger.info(f"ğŸ”§ æŠ–éŸ³åˆ›ä½œè€…IDåˆ—è¡¨: {','.join(task.creator_ids)}")
                
                elif platform_str == "ks":
                    cmd.extend(["--ks_creator_ids", ";".join(task.creator_ids)])
                    logger.info(f"ğŸ”§ å¿«æ‰‹åˆ›ä½œè€…IDåˆ—è¡¨: {','.join(task.creator_ids)}")
                
                elif platform_str == "bili":
                    cmd.extend(["--bili_creator_ids", ";".join(task.creator_ids)])
                    logger.info(f"ğŸ”§ Bç«™åˆ›ä½œè€…IDåˆ—è¡¨: {','.join(task.creator_ids)}")
                
                elif platform_str == "wb":
                    cmd.extend(["--weibo_creator_ids", ";".join(task.creator_ids)])
                    logger.info(f"ğŸ”§ å¾®åšåˆ›ä½œè€…IDåˆ—è¡¨: {','.join(task.creator_ids)}")
                
                elif platform_str == "zhihu":
                    # çŸ¥ä¹å’Œè´´å§ä½¿ç”¨URLæ ¼å¼
                    cmd.extend(["--zhihu_creator_urls", ";".join(task.creator_ids)])
                    logger.info(f"ğŸ”§ çŸ¥ä¹åˆ›ä½œè€…URLåˆ—è¡¨: {','.join(task.creator_ids)}")
                
                elif platform_str == "tieba":
                    cmd.extend(["--tieba_creator_urls", ";".join(task.creator_ids)])
                    logger.info(f"ğŸ”§ è´´å§åˆ›ä½œè€…URLåˆ—è¡¨: {','.join(task.creator_ids)}")
            else:
                logger.warning("âš ï¸  Creatoræ¨¡å¼éœ€è¦æä¾›creator_idså‚æ•°")
        
        logger.info(f"ğŸš€ æ„å»ºçš„å‘½ä»¤: {' '.join(cmd)}")
        return cmd
    
    async def _execute_crawler_process(self, cmd: List[str], task_logger) -> Dict[str, Any]:
        """æ‰§è¡Œçˆ¬è™«è¿›ç¨‹å¹¶å®æ—¶ç›‘æ§è¿›åº¦"""
        try:
            # å¼‚æ­¥æ‰§è¡Œå­è¿›ç¨‹
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                cwd=MEDIACRAWLER_PATH
            )
            
            # å®æ—¶è¯»å–è¾“å‡ºå¹¶è§£æè¿›åº¦
            stdout_lines = []
            stderr_lines = []
            
            async def read_stdout():
                """è¯»å–æ ‡å‡†è¾“å‡ºå¹¶è§£æè¿›åº¦"""
                async for line in process.stdout:
                    line_text = line.decode('utf-8', errors='ignore').strip()
                    if line_text:
                        stdout_lines.append(line_text)
                        # è§£æè¿›åº¦ä¿¡æ¯
                        await self._parse_progress_from_line(line_text, task_logger)
            
            async def read_stderr():
                """è¯»å–é”™è¯¯è¾“å‡º"""
                async for line in process.stderr:
                    line_text = line.decode('utf-8', errors='ignore').strip()
                    if line_text:
                        stderr_lines.append(line_text)
                        # è®°å½•é”™è¯¯æ—¥å¿—
                        task_logger.log_event(
                            TaskEventType.CRAWLER_ERROR,
                            f"MediaCrawler stderr: {line_text}"
                        )
            
            # å¹¶å‘è¯»å–stdoutå’Œstderr
            await asyncio.gather(read_stdout(), read_stderr())
            
            # ç­‰å¾…è¿›ç¨‹ç»“æŸ
            returncode = await process.wait()
            
            # åˆå¹¶è¾“å‡º
            stdout_text = "\n".join(stdout_lines)
            stderr_text = "\n".join(stderr_lines)
            
            if returncode == 0:
                # ä»è¾“å‡ºä¸­è§£ææœ€ç»ˆæ•°æ®æ•°é‡
                data_count = self._parse_data_count_from_output(stdout_text)
                
                # ğŸª å°è¯•æå–å¹¶ä¿å­˜æ–°çš„cookies
                await self._extract_and_save_cookies(task.platform, stdout_text, stderr_text, task.task_id)
                
                task_logger.log_event(
                    TaskEventType.TASK_PROGRESS,
                    "MediaCrawleræ‰§è¡ŒæˆåŠŸ",
                    data={"final_data_count": data_count}
                )
                
                return {
                    "success": True,
                    "data_count": data_count,
                    "stdout": stdout_text,
                    "stderr": stderr_text
                }
            else:
                task_logger.log_event(
                    TaskEventType.CRAWLER_ERROR,
                    f"MediaCrawleræ‰§è¡Œå¤±è´¥: é€€å‡ºç ={returncode}",
                    error=stderr_text
                )
                
                return {
                    "success": False,
                    "error": f"è¿›ç¨‹é€€å‡ºç : {returncode}, é”™è¯¯: {stderr_text}",
                    "stdout": stdout_text,
                    "stderr": stderr_text
                }
                
        except Exception as e:
            task_logger.log_event(
                TaskEventType.CRAWLER_ERROR,
                f"æ‰§è¡ŒMediaCrawleræ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}",
                error=str(e)
            )
            
            return {
                "success": False,
                "error": f"æ‰§è¡Œå¼‚å¸¸: {str(e)}"
            }
    
    async def _parse_progress_from_line(self, line: str, task_logger) -> None:
        """ä»è¾“å‡ºè¡Œä¸­è§£æå®æ—¶è¿›åº¦"""
        import re
        
        try:
            # è§£æä¸åŒç±»å‹çš„è¿›åº¦ä¿¡æ¯
            
            # 1. ç™»å½•ç›¸å…³è¿›åº¦
            login_patterns = [
                (r"å¼€å§‹ç™»å½•", "logging_in", 20.0),
                (r"ç™»å½•æˆåŠŸ", "logged_in", 30.0),
                (r"æ‰«ç ç™»å½•", "qrcode_login", 25.0),
                (r"æ‰‹æœºç™»å½•", "phone_login", 25.0),
            ]
            
            for pattern, stage, percent in login_patterns:
                if re.search(pattern, line, re.IGNORECASE):
                    task_logger.update_progress(stage, percent)
                    return
            
            # 2. çˆ¬å–è¿›åº¦ç›¸å…³
            crawl_patterns = [
                (r"å¼€å§‹çˆ¬å–", "crawling", 40.0),
                (r"æ­£åœ¨çˆ¬å–ç¬¬\s*(\d+)\s*é¡µ", "crawling", None),  # åŠ¨æ€è®¡ç®—
                (r"å·²çˆ¬å–\s*(\d+)\s*/\s*(\d+)", "crawling", None),  # è¿›åº¦æ¯”ä¾‹
                (r"çˆ¬å–.*?(\d+)\s*æ¡.*?å…±\s*(\d+)", "crawling", None),  # å·²å®Œæˆ/æ€»æ•°
            ]
            
            for pattern, stage, percent in crawl_patterns:
                match = re.search(pattern, line, re.IGNORECASE)
                if match:
                    if percent is not None:
                        task_logger.update_progress(stage, percent)
                    else:
                        # åŠ¨æ€è®¡ç®—è¿›åº¦
                        if "å·²çˆ¬å–" in pattern and len(match.groups()) >= 2:
                            completed = int(match.group(1))
                            total = int(match.group(2))
                            if total > 0:
                                progress_percent = min(40.0 + (completed / total) * 50.0, 90.0)
                                task_logger.update_progress(
                                    stage, progress_percent, 
                                    items_total=total, 
                                    items_completed=completed
                                )
                    return
            
            # 3. æ•°æ®ä¿å­˜è¿›åº¦
            save_patterns = [
                (r"å¼€å§‹ä¿å­˜", "saving", 90.0),
                (r"ä¿å­˜.*?(\d+)\s*æ¡", "saving", 95.0),
                (r"ä¿å­˜å®Œæˆ", "completed", 100.0),
            ]
            
            for pattern, stage, percent in save_patterns:
                match = re.search(pattern, line, re.IGNORECASE)
                if match:
                    task_logger.update_progress(stage, percent)
                    if len(match.groups()) >= 1:
                        try:
                            saved_count = int(match.group(1))
                            task_logger.update_progress(
                                stage, percent,
                                items_completed=saved_count
                            )
                        except:
                            pass
                    return
            
            # 4. é”™è¯¯ä¿¡æ¯
            error_patterns = [
                r"é”™è¯¯",
                r"å¤±è´¥",
                r"å¼‚å¸¸",
                r"Error",
                r"Exception"
            ]
            
            for pattern in error_patterns:
                if re.search(pattern, line, re.IGNORECASE):
                    task_logger.log_event(
                        TaskEventType.CRAWLER_ERROR,
                        f"MediaCrawleræŠ¥å‘Šé”™è¯¯: {line}"
                    )
                    return
            
            # 5. è®°å½•é‡è¦ä¿¡æ¯è¡Œ
            important_patterns = [
                r"å¼€å§‹",
                r"å®Œæˆ",
                r"æˆåŠŸ",
                r"å…³é”®è¯",
                r"ç”¨æˆ·",
                r"å†…å®¹"
            ]
            
            for pattern in important_patterns:
                if re.search(pattern, line, re.IGNORECASE):
                    task_logger.log_event(
                        TaskEventType.TASK_PROGRESS,
                        f"MediaCrawler: {line}"
                    )
                    return
                    
        except Exception as e:
            # è§£æå¤±è´¥ä¸å½±å“ä¸»æµç¨‹
            pass
    
    def _parse_data_count_from_output(self, output: str) -> int:
        """ä»è¾“å‡ºä¸­è§£ææ•°æ®æ•°é‡"""
        try:
            import re
            # æŸ¥æ‰¾ç±»ä¼¼ "å…±çˆ¬å– 123 æ¡æ•°æ®" çš„æ¨¡å¼
            patterns = [
                r"å…±çˆ¬å–\s*(\d+)\s*æ¡",
                r"è·å–\s*(\d+)\s*æ¡æ•°æ®",
                r"crawled\s*(\d+)\s*items",
                r"total[:\s]*(\d+)",
                r"ä¿å­˜.*?(\d+)\s*æ¡",
                r"å®Œæˆ.*?(\d+)\s*ä¸ª"
            ]
            
            for pattern in patterns:
                matches = re.findall(pattern, output, re.IGNORECASE)
                if matches:
                    # è¿”å›æœ€å¤§çš„æ•°å­—ï¼ˆé€šå¸¸æ˜¯æœ€ç»ˆç»“æœï¼‰
                    return max(int(match) for match in matches)
            
            return 0
        except Exception:
            return 0
    
    async def get_task_status(self, task_id: str) -> Dict[str, Any]:
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        base_status = {"task_id": task_id}
        
        if task_id in self.running_tasks:
            task = self.running_tasks[task_id]
            base_status.update({
                "status": "running" if not task.done() else "completed",
                "done": task.done()
            })
        elif task_id in self.task_results:
            result = self.task_results[task_id]
            base_status.update({
                "status": "completed",
                "done": True,
                "success": result.success,
                "message": result.message,
                "data_count": result.data_count,
                "error_count": result.error_count
            })
        else:
            base_status.update({
                "status": "not_found",
                "done": False
            })
        
        # è·å–è¿›åº¦ä¿¡æ¯
        progress = logging_manager.get_task_progress(task_id)
        if progress:
            base_status.update({
                "progress": {
                    "current_stage": progress.current_stage,
                    "progress_percent": progress.progress_percent,
                    "items_total": progress.items_total,
                    "items_completed": progress.items_completed,
                    "items_failed": progress.items_failed,
                    "current_item": progress.current_item,
                    "estimated_remaining_time": progress.estimated_remaining_time,
                    "last_update": progress.last_update
                }
            })
        
        return base_status
    
    async def get_task_result(self, task_id: str) -> Optional[CrawlerResult]:
        """è·å–ä»»åŠ¡ç»“æœ"""
        return self.task_results.get(task_id)
    
    async def stop_task(self, task_id: str) -> bool:
        """åœæ­¢ä»»åŠ¡"""
        if task_id in self.running_tasks:
            task = self.running_tasks[task_id]
            task.cancel()
            
            try:
                await task
            except asyncio.CancelledError:
                pass
            
            self.task_results[task_id] = CrawlerResult(
                task_id=task_id,
                success=False,
                message="ä»»åŠ¡å·²è¢«æ‰‹åŠ¨åœæ­¢"
            )
            
            del self.running_tasks[task_id]
            logger.info(f"ä»»åŠ¡ {task_id} å·²åœæ­¢")
            return True
        
        return False
    
    async def list_running_tasks(self) -> List[str]:
        """åˆ—å‡ºè¿è¡Œä¸­çš„ä»»åŠ¡"""
        return list(self.running_tasks.keys())
    
    async def get_task_events(self, task_id: str, limit: int = 50) -> List[Dict[str, Any]]:
        """è·å–ä»»åŠ¡äº‹ä»¶æ—¥å¿—"""
        events = logging_manager.get_task_events(task_id, limit)
        return [
            {
                "task_id": event.task_id,
                "event_type": event.event_type.value,
                "timestamp": event.timestamp,
                "message": event.message,
                "data": event.data,
                "platform": event.platform,
                "progress": event.progress,
                "error": event.error
            }
            for event in events
        ]
    
    async def get_system_stats(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        running_count = len(self.running_tasks)
        completed_count = len(self.task_results)
        
        # æˆåŠŸ/å¤±è´¥ç»Ÿè®¡
        success_count = sum(1 for result in self.task_results.values() if result.success)
        failed_count = completed_count - success_count
        
        # æ•°æ®ç»Ÿè®¡
        total_data_count = sum(result.data_count for result in self.task_results.values())
        total_error_count = sum(result.error_count for result in self.task_results.values())
        
        # ä»æ—¥å¿—ç®¡ç†å™¨è·å–é¢å¤–ç»Ÿè®¡
        logging_stats = logging_manager.get_system_stats()
        
        return {
            "tasks": {
                "running": running_count,
                "completed": completed_count,
                "success": success_count,
                "failed": failed_count
            },
            "data": {
                "total_crawled": total_data_count,
                "total_errors": total_error_count
            },
            "system": {
                "uptime": logging_stats.get("uptime", 0),
                "memory_usage": logging_stats.get("memory_usage", 0),
                "active_loggers": logging_stats.get("active_loggers", 0)
            }
        }
    
    async def cleanup_completed_tasks(self) -> Dict[str, Any]:
        """æ¸…ç†å·²å®Œæˆçš„ä»»åŠ¡"""
        before_count = len(self.task_results)
        
        # ä¿ç•™æœ€è¿‘100ä¸ªä»»åŠ¡ç»“æœï¼Œåˆ é™¤å…¶ä½™çš„
        if before_count > 100:
            # æŒ‰ä»»åŠ¡IDæ’åºï¼Œä¿ç•™æœ€æ–°çš„100ä¸ª
            sorted_tasks = sorted(self.task_results.items(), key=lambda x: x[0])
            tasks_to_remove = sorted_tasks[:-100]
            
            for task_id, _ in tasks_to_remove:
                del self.task_results[task_id]
                # æ¸…ç†å¯¹åº”çš„ä»»åŠ¡æ—¥å¿—
                logging_manager.cleanup_task_logger(task_id)
        
        after_count = len(self.task_results)
        cleaned_count = before_count - after_count
        
        logger.info(f"æ¸…ç†å®Œæˆï¼šåˆ é™¤äº† {cleaned_count} ä¸ªå·²å®Œæˆçš„ä»»åŠ¡")
        
        return {
            "cleaned_tasks": cleaned_count,
            "remaining_tasks": after_count,
            "message": f"å·²æ¸…ç† {cleaned_count} ä¸ªå·²å®Œæˆçš„ä»»åŠ¡"
        }
    
    def _get_platform_string(self, platform: PlatformType) -> str:
        """è·å–å¹³å°å­—ç¬¦ä¸²"""
        platform_mapping = {
            PlatformType.XHS: "xhs",
            PlatformType.DOUYIN: "dy",
            PlatformType.BILIBILI: "bili",
            PlatformType.KUAISHOU: "ks",
            PlatformType.WEIBO: "wb",
            PlatformType.TIEBA: "tieba",
            PlatformType.ZHIHU: "zhihu"
        }
        return platform_mapping.get(platform, str(platform))

    async def _extract_and_save_cookies(self, platform: PlatformType, stdout_text: str, stderr_text: str, task_id: str) -> None:
        """å°è¯•ä»MediaCrawlerçš„browser_dataç›®å½•ä¸­è¯»å–å¹¶ä¿å­˜æ–°çš„cookies"""
        try:
            platform_str = self._get_platform_string(platform)
            
            # MediaCrawlerä¿å­˜cookiesçš„è·¯å¾„
            browser_data_path = Path(MEDIACRAWLER_PATH) / "browser_data"
            cookies_pattern = f"{platform_str}_cookies_*.json"
            
            # æŸ¥æ‰¾æœ€æ–°çš„cookiesæ–‡ä»¶
            cookies_files = list(browser_data_path.glob(cookies_pattern))
            if cookies_files:
                # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„
                latest_cookies_file = max(cookies_files, key=lambda f: f.stat().st_mtime)
                
                with open(latest_cookies_file, 'r', encoding='utf-8') as f:
                    cookies_data = json.load(f)
                
                # å°†cookiesè½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼
                if isinstance(cookies_data, list):
                    # å¦‚æœæ˜¯cookiesæ•°ç»„ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
                    cookie_strings = []
                    for cookie in cookies_data:
                        if isinstance(cookie, dict) and 'name' in cookie and 'value' in cookie:
                            cookie_strings.append(f"{cookie['name']}={cookie['value']}")
                    
                    if cookie_strings:
                        cookies_str = "; ".join(cookie_strings)
                        # ä¿å­˜åˆ°æˆ‘ä»¬çš„cookiesç®¡ç†å™¨
                        success = cookies_manager.save_cookies(platform_str, cookies_str, task_id)
                        if success:
                            logger.info(f"ğŸª æˆåŠŸä¿å­˜cookies [{platform_str}]: {len(cookie_strings)}ä¸ªcookie")
                        else:
                            logger.warning(f"âš ï¸  ä¿å­˜cookieså¤±è´¥ [{platform_str}]")
                    else:
                        logger.warning(f"âš ï¸  Cookiesæ•°æ®æ ¼å¼å¼‚å¸¸ [{platform_str}]")
                else:
                    logger.warning(f"âš ï¸  Cookiesæ•°æ®ä¸æ˜¯æ•°ç»„æ ¼å¼ [{platform_str}]")
            else:
                logger.info(f"ğŸ“„ æœªæ‰¾åˆ°cookiesæ–‡ä»¶ [{platform_str}]: {cookies_pattern}")
                
        except Exception as e:
            logger.error(f"âŒ æå–å¹¶ä¿å­˜cookieså¤±è´¥ [{platform}]: {e}")


# å…¨å±€é€‚é…å™¨å®ä¾‹
crawler_adapter = MediaCrawlerAdapter() 