"""
MediaCrawler API Server

åŸºäºåŸMediaCrawleré¡¹ç›®çš„FastAPIæœåŠ¡ï¼Œ
é€šè¿‡é€‚é…å™¨æ¨¡å¼å¤ç”¨åŸæœ‰çš„çˆ¬è™«åŠŸèƒ½ã€‚
"""

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
import uuid
import logging

from app.crawler.adapter import (
    crawler_adapter, 
    CrawlerTask, 
    CrawlerTaskType,
    CrawlerResult
)
from app.dataReader.base import PlatformType
from app.core.config_manager import CrawlerConfigRequest, get_config_manager
from app.core.logging import logging_manager
from app.api.login import router as login_router
from app.api.data import router as data_router
from app.dataReader.factory import DataReaderFactory
from app.dataReader.base import DataSourceType

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="MediaCrawler API Server",
    description="åŸºäºMediaCrawlerçš„ç¤¾äº¤åª’ä½“æ•°æ®é‡‡é›†APIæœåŠ¡",
    version="1.0.0"
)

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ³¨å†Œè·¯ç”±
app.include_router(login_router)
app.include_router(data_router)


# åº”ç”¨ç”Ÿå‘½å‘¨æœŸäº‹ä»¶
@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ–"""
    logger.info("Initializing MediaCrawler API Server...")
    
    # åˆå§‹åŒ–æ•°æ®è®¿é—®ç®¡ç†å™¨
    try:
        # é¢„çƒ­æ•°æ®è¯»å–å™¨ï¼ˆåˆ›å»ºä¸€ä¸ªé»˜è®¤å®ä¾‹ï¼‰
        await DataReaderFactory.create_data_reader(DataSourceType.JSON, PlatformType.XHS)
        logger.info("Data access manager initialized successfully")
    except Exception as e:
        logger.error(f"Failed to initialize data access manager: {e}")
        # ä¸é˜»æ­¢åº”ç”¨å¯åŠ¨ï¼Œå…è®¸é™çº§æœåŠ¡
    
    logger.info("MediaCrawler API Server startup complete")


@app.on_event("shutdown")
async def shutdown_event():
    """åº”ç”¨å…³é—­æ—¶æ¸…ç†"""
    logger.info("Shutting down MediaCrawler API Server...")
    
    try:
        await DataReaderFactory.close_all()
        logger.info("Data access manager closed successfully")
    except Exception as e:
        logger.error(f"Error during data access manager shutdown: {e}")
    
    logger.info("MediaCrawler API Server shutdown complete")


# Pydanticæ¨¡å‹å®šä¹‰
class CrawlerTaskRequest(BaseModel):
    """çˆ¬è™«ä»»åŠ¡è¯·æ±‚"""
    platform: str = Field(..., description="å¹³å°åç§°(xhs, dy, ks, bili, wb, zhihu, tieba)")
    task_type: str = Field(..., description="ä»»åŠ¡ç±»å‹(search, detail, creator)")
    keywords: Optional[List[str]] = None
    content_ids: Optional[List[str]] = None
    creator_ids: Optional[List[str]] = None
    max_count: int = Field(default=100, ge=1, le=1000)
    max_comments: int = Field(default=50, ge=0, le=500)
    start_page: int = Field(default=1, ge=1)
    enable_proxy: bool = False
    headless: bool = True
    enable_comments: bool = True
    enable_sub_comments: bool = False
    save_data_option: str = Field(default="db", pattern="^(db|json|csv)$")
    config: Optional[CrawlerConfigRequest] = None
    clear_cookies: bool = Field(default=False, description="æ˜¯å¦æ¸…é™¤cookiesé‡æ–°ç™»å½•")


class CrawlerTaskResponse(BaseModel):
    task_id: str
    message: str


class TaskProgressInfo(BaseModel):
    current_stage: str
    progress_percent: float
    items_total: int
    items_completed: int
    items_failed: int
    current_item: Optional[str] = None
    estimated_remaining_time: Optional[int] = None
    last_update: Optional[str] = None


class TaskStatusResponse(BaseModel):
    task_id: str
    status: str  # "running", "completed", "not_found"
    done: bool
    success: Optional[bool] = None
    message: Optional[str] = None
    data_count: Optional[int] = None
    error_count: Optional[int] = None
    progress: Optional[TaskProgressInfo] = None


class TaskResultResponse(BaseModel):
    task_id: str
    success: bool
    message: str
    data_count: int = 0
    error_count: int = 0
    data: Optional[List[Dict]] = None
    errors: Optional[List[str]] = None


# å¹³å°æ˜ å°„
PLATFORM_MAPPING = {
    "xhs": PlatformType.XHS,
    "douyin": PlatformType.DOUYIN,
    "bilibili": PlatformType.BILIBILI,
    "kuaishou": PlatformType.KUAISHOU,
    "weibo": PlatformType.WEIBO,
    "tieba": PlatformType.TIEBA,
    "zhihu": PlatformType.ZHIHU
}


@app.get("/")
async def root():
    """å¥åº·æ£€æŸ¥"""
    return {
        "message": "MediaCrawler API Server is running",
        "version": "1.0.0",
        "supported_platforms": list(PLATFORM_MAPPING.keys())
    }


@app.post("/api/v1/tasks", response_model=CrawlerTaskResponse)
async def create_crawler_task(
    request: CrawlerTaskRequest,
    background_tasks: BackgroundTasks
):
    """åˆ›å»ºçˆ¬è™«ä»»åŠ¡"""
    try:
        # éªŒè¯å¹³å°
        if request.platform not in PLATFORM_MAPPING:
            raise HTTPException(
                status_code=400,
                detail=f"ä¸æ”¯æŒçš„å¹³å°: {request.platform}"
            )
        
        # éªŒè¯ä»»åŠ¡ç±»å‹
        try:
            task_type = CrawlerTaskType(request.task_type)
        except ValueError:
            raise HTTPException(
                status_code=400,
                detail=f"ä¸æ”¯æŒçš„ä»»åŠ¡ç±»å‹: {request.task_type}"
            )
        
        # å‚æ•°éªŒè¯
        if request.task_type == "search" and not request.keywords:
            raise HTTPException(status_code=400, detail="æœç´¢æ¨¡å¼éœ€è¦æä¾›keywordså‚æ•°")
        
        if request.task_type == "detail" and not request.content_ids:
            raise HTTPException(status_code=400, detail="è¯¦æƒ…æ¨¡å¼éœ€è¦æä¾›content_idså‚æ•°")
        
        if request.task_type == "creator" and not request.creator_ids:
            raise HTTPException(status_code=400, detail="åˆ›ä½œè€…æ¨¡å¼éœ€è¦æä¾›creator_idså‚æ•°")

        # ğŸª å¤„ç†cookiesæ¸…é™¤è¯·æ±‚
        if request.clear_cookies:
            from app.core.cookies_manager import cookies_manager
            platform_str = crawler_adapter._get_platform_string(PlatformType(request.platform))
            success = cookies_manager.clear_cookies(platform_str)
            logger.info(f"ğŸ—‘ï¸  æ¸…é™¤cookies {'æˆåŠŸ' if success else 'å¤±è´¥'}: {platform_str}")

        # ç”Ÿæˆä»»åŠ¡ID
        task_id = str(uuid.uuid4())
        
        # åˆ›å»ºä»»åŠ¡
        task = CrawlerTask(
            task_id=task_id,
            platform=PLATFORM_MAPPING[request.platform],
            task_type=task_type,
            keywords=request.keywords,
            content_ids=request.content_ids,
            creator_ids=request.creator_ids,
            max_count=request.max_count,
            max_comments=request.max_comments,
            start_page=request.start_page,
            enable_proxy=request.enable_proxy,
            headless=request.headless,
            enable_comments=request.enable_comments,
            enable_sub_comments=request.enable_sub_comments,
            save_data_option=request.save_data_option,
            config=request.config,
            clear_cookies=request.clear_cookies
        )
        
        # å¯åŠ¨ä»»åŠ¡
        await crawler_adapter.start_crawler_task(task)
        
        return CrawlerTaskResponse(
            task_id=task_id,
            message="ä»»åŠ¡å·²åˆ›å»ºå¹¶å¼€å§‹æ‰§è¡Œ"
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ›å»ºçˆ¬è™«ä»»åŠ¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/v1/tasks/{task_id}/status", response_model=TaskStatusResponse)
async def get_task_status(task_id: str):
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    try:
        status = await crawler_adapter.get_task_status(task_id)
        return TaskStatusResponse(**status)
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/v1/tasks/{task_id}/result", response_model=TaskResultResponse)
async def get_task_result(task_id: str):
    """è·å–ä»»åŠ¡ç»“æœ"""
    try:
        result = await crawler_adapter.get_task_result(task_id)
        
        if result is None:
            raise HTTPException(
                status_code=404,
                detail="ä»»åŠ¡ä¸å­˜åœ¨"
            )
        
        return TaskResultResponse(
            task_id=result.task_id,
            success=result.success,
            message=result.message,
            data_count=result.data_count,
            error_count=result.error_count,
            data=result.data,
            errors=result.errors
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡ç»“æœå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.delete("/api/v1/tasks/{task_id}")
async def stop_task(task_id: str):
    """åœæ­¢ä»»åŠ¡"""
    try:
        success = await crawler_adapter.stop_task(task_id)
        
        if not success:
            raise HTTPException(
                status_code=404,
                detail="ä»»åŠ¡ä¸å­˜åœ¨æˆ–å·²å®Œæˆ"
            )
        
        return {"message": "ä»»åŠ¡å·²åœæ­¢"}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åœæ­¢ä»»åŠ¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/v1/tasks")
async def list_running_tasks():
    """åˆ—å‡ºè¿è¡Œä¸­çš„ä»»åŠ¡"""
    try:
        running_tasks = await crawler_adapter.list_running_tasks()
        return {
            "running_tasks": running_tasks,
            "count": len(running_tasks)
        }
    except Exception as e:
        logger.error(f"åˆ—å‡ºè¿è¡Œä»»åŠ¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/v1/tasks/{task_id}/events")
async def get_task_events(task_id: str, limit: int = 50):
    """è·å–ä»»åŠ¡äº‹ä»¶æ—¥å¿—"""
    try:
        events = await crawler_adapter.get_task_events(task_id, limit)
        return {
            "task_id": task_id,
            "events": events,
            "count": len(events)
        }
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡äº‹ä»¶å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ä»»åŠ¡äº‹ä»¶å¤±è´¥: {str(e)}")


@app.get("/api/v1/system/stats")
async def get_system_stats():
    """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
    try:
        stats = await crawler_adapter.get_system_stats()
        return stats
    except Exception as e:
        logger.error(f"è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/v1/system/config/options")
async def get_config_options():
    """è·å–æ”¯æŒçš„é…ç½®é€‰é¡¹"""
    try:
        config_manager = get_config_manager()
        options = config_manager.get_supported_config_options()
        return {
            "message": "æ”¯æŒçš„é…ç½®é€‰é¡¹",
            "options": options
        }
    except Exception as e:
        logger.error(f"è·å–é…ç½®é€‰é¡¹å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/v1/maintenance/cleanup")
async def cleanup_completed_tasks():
    """æ¸…ç†å·²å®Œæˆçš„ä»»åŠ¡ç»“æœ"""
    try:
        await crawler_adapter.cleanup_completed_tasks()
        return {"message": "æ¸…ç†å®Œæˆ"}
    except Exception as e:
        logger.error(f"æ¸…ç†ä»»åŠ¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ä¾¿æ·çš„å¹³å°ç‰¹å®šç«¯ç‚¹
class QuickSearchRequest(BaseModel):
    keywords: List[str]
    max_count: int = 100
    max_comments: int = 50
    enable_proxy: bool = False
    config: Optional[CrawlerConfigRequest] = None


@app.post("/api/v1/xhs/search")
async def xhs_search(request: QuickSearchRequest):
    """å°çº¢ä¹¦æœç´¢"""
    task_id = str(uuid.uuid4())
    task = CrawlerTask(
        task_id=task_id,
        platform=PlatformType.XHS,
        task_type=CrawlerTaskType.SEARCH,
        keywords=request.keywords,
        max_count=request.max_count,
        max_comments=request.max_comments,
        enable_proxy=request.enable_proxy,
        config=request.config
    )
    
    await crawler_adapter.start_crawler_task(task)
    return {"task_id": task_id, "message": "å°çº¢ä¹¦æœç´¢ä»»åŠ¡å·²å¯åŠ¨"}


@app.post("/api/v1/douyin/search")
async def douyin_search(request: QuickSearchRequest):
    """æŠ–éŸ³æœç´¢"""
    task_id = str(uuid.uuid4())
    task = CrawlerTask(
        task_id=task_id,
        platform=PlatformType.DOUYIN,
        task_type=CrawlerTaskType.SEARCH,
        keywords=request.keywords,
        max_count=request.max_count,
        max_comments=request.max_comments,
        enable_proxy=request.enable_proxy,
        config=request.config
    )
    
    await crawler_adapter.start_crawler_task(task)
    return {"task_id": task_id, "message": "æŠ–éŸ³æœç´¢ä»»åŠ¡å·²å¯åŠ¨"}


# ===== Cookiesç®¡ç†API =====

@app.get("/api/v1/cookies/{platform}/status")
async def get_cookies_status(platform: str):
    """è·å–æŒ‡å®šå¹³å°çš„cookiesçŠ¶æ€"""
    try:
        from app.core.cookies_manager import cookies_manager
        
        status = cookies_manager.get_cookies_status(platform, max_age_days=7)
        return {
            "success": True,
            "data": status,
            "message": f"CookiesçŠ¶æ€è·å–æˆåŠŸ: {platform}"
        }
    except Exception as e:
        logger.error(f"è·å–cookiesçŠ¶æ€å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–cookiesçŠ¶æ€å¤±è´¥: {str(e)}")


@app.get("/api/v1/cookies")
async def list_all_cookies():
    """åˆ—å‡ºæ‰€æœ‰å¹³å°çš„cookiesç¼“å­˜ä¿¡æ¯"""
    try:
        from app.core.cookies_manager import cookies_manager
        
        cookies_info = cookies_manager.list_cached_cookies()
        return {
            "success": True,
            "data": cookies_info,
            "count": len(cookies_info),
            "message": "Cookiesåˆ—è¡¨è·å–æˆåŠŸ"
        }
    except Exception as e:
        logger.error(f"è·å–cookiesåˆ—è¡¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–cookiesåˆ—è¡¨å¤±è´¥: {str(e)}")


@app.delete("/api/v1/cookies/{platform}")
async def clear_platform_cookies(platform: str):
    """æ¸…é™¤æŒ‡å®šå¹³å°çš„cookies"""
    try:
        from app.core.cookies_manager import cookies_manager
        
        success = cookies_manager.clear_cookies(platform)
        return {
            "success": success,
            "platform": platform,
            "message": f"Cookiesæ¸…é™¤{'æˆåŠŸ' if success else 'å¤±è´¥'}: {platform}"
        }
    except Exception as e:
        logger.error(f"æ¸…é™¤cookieså¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ¸…é™¤cookieså¤±è´¥: {str(e)}")


@app.delete("/api/v1/cookies")
async def clear_all_cookies():
    """æ¸…é™¤æ‰€æœ‰å¹³å°çš„cookies"""
    try:
        from app.core.cookies_manager import cookies_manager
        
        success = cookies_manager.clear_cookies()
        return {
            "success": success,
            "message": f"æ‰€æœ‰cookiesæ¸…é™¤{'æˆåŠŸ' if success else 'å¤±è´¥'}"
        }
    except Exception as e:
        logger.error(f"æ¸…é™¤æ‰€æœ‰cookieså¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ¸…é™¤æ‰€æœ‰cookieså¤±è´¥: {str(e)}")


class SaveCookiesRequest(BaseModel):
    """ä¿å­˜cookiesè¯·æ±‚"""
    platform: str = Field(..., description="å¹³å°åç§°")
    cookies: str = Field(..., description="Cookieså­—ç¬¦ä¸²")
    task_id: Optional[str] = Field(None, description="ä»»åŠ¡ID")


@app.post("/api/v1/cookies")
async def save_cookies(request: SaveCookiesRequest):
    """æ‰‹åŠ¨ä¿å­˜cookies"""
    try:
        from app.core.cookies_manager import cookies_manager
        
        success = cookies_manager.save_cookies(
            request.platform, 
            request.cookies, 
            request.task_id
        )
        
        return {
            "success": success,
            "platform": request.platform,
            "task_id": request.task_id,
            "message": f"Cookiesä¿å­˜{'æˆåŠŸ' if success else 'å¤±è´¥'}: {request.platform}"
        }
    except Exception as e:
        logger.error(f"ä¿å­˜cookieså¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ä¿å­˜cookieså¤±è´¥: {str(e)}")


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000) 