"""
MediaCrawler é€‚é…å™¨æµ‹è¯•è„šæœ¬

æµ‹è¯•é€šè¿‡è¿›ç¨‹æ–¹å¼è°ƒç”¨åŸMediaCrawleré¡¹ç›®çš„åŠŸèƒ½
"""

import asyncio
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))

from app.crawler.adapter import (
    crawler_adapter, 
    CrawlerTask, 
    CrawlerTaskType
)
from app.dataReader.base import PlatformType


async def test_crawler_adapter():
    """æµ‹è¯•çˆ¬è™«é€‚é…å™¨"""
    
    print("ğŸš€ å¼€å§‹æµ‹è¯•MediaCrawleré€‚é…å™¨")
    
    # åˆ›å»ºæµ‹è¯•ä»»åŠ¡
    task = CrawlerTask(
        task_id="test_001",
        platform=PlatformType.XHS,
        task_type=CrawlerTaskType.SEARCH,
        keywords=["ç¾é£Ÿ", "æ—…è¡Œ"],
        max_count=5,
        max_comments=10,
        headless=True,
        save_data_option="json"  # ä½¿ç”¨jsonä¿å­˜ï¼Œé¿å…æ•°æ®åº“ä¾èµ–
    )
    
    print(f"ğŸ“‹ åˆ›å»ºä»»åŠ¡: {task.task_id}")
    print(f"   å¹³å°: {task.platform.value}")
    print(f"   ç±»å‹: {task.task_type.value}")
    print(f"   å…³é”®è¯: {task.keywords}")
    print(f"   æœ€å¤§æ•°é‡: {task.max_count}")
    
    try:
        # å¯åŠ¨ä»»åŠ¡
        task_id = await crawler_adapter.start_crawler_task(task)
        print(f"âœ… ä»»åŠ¡å¯åŠ¨æˆåŠŸ: {task_id}")
        
        # ç›‘æ§ä»»åŠ¡çŠ¶æ€
        print("\nğŸ“Š ç›‘æ§ä»»åŠ¡çŠ¶æ€:")
        while True:
            status = await crawler_adapter.get_task_status(task_id)
            print(f"   çŠ¶æ€: {status.get('status')}")
            
            if status.get('progress'):
                progress = status['progress']
                print(f"   è¿›åº¦: {progress.get('stage')} - {progress.get('percentage', 0):.1f}%")
            
            if status.get('done'):
                break
                
            await asyncio.sleep(2)
        
        # è·å–æœ€ç»ˆç»“æœ
        result = await crawler_adapter.get_task_result(task_id)
        if result:
            print(f"\nğŸ“ˆ ä»»åŠ¡ç»“æœ:")
            print(f"   æˆåŠŸ: {result.success}")
            print(f"   æ¶ˆæ¯: {result.message}")
            print(f"   æ•°æ®æ•°é‡: {result.data_count}")
            
            if result.errors:
                print(f"   é”™è¯¯: {result.errors}")
        
        # è·å–ä»»åŠ¡äº‹ä»¶æ—¥å¿—
        events = await crawler_adapter.get_task_events(task_id, limit=10)
        print(f"\nğŸ“ ä»»åŠ¡äº‹ä»¶æ—¥å¿— (æœ€è¿‘10æ¡):")
        for event in events[-5:]:  # æ˜¾ç¤ºæœ€å5æ¡
            print(f"   [{event['timestamp']}] {event['event_type']}: {event['message']}")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


async def test_multiple_tasks():
    """æµ‹è¯•å¤šä»»åŠ¡å¹¶å‘"""
    
    print("\nğŸ”¥ æµ‹è¯•å¤šä»»åŠ¡å¹¶å‘")
    
    tasks = [
        CrawlerTask(
            task_id=f"multi_test_{i}",
            platform=PlatformType.XHS,
            task_type=CrawlerTaskType.SEARCH,
            keywords=[f"æµ‹è¯•å…³é”®è¯{i}"],
            max_count=3,
            headless=True,
            save_data_option="json"
        )
        for i in range(1, 4)
    ]
    
    # å¯åŠ¨æ‰€æœ‰ä»»åŠ¡
    task_ids = []
    for task in tasks:
        try:
            task_id = await crawler_adapter.start_crawler_task(task)
            task_ids.append(task_id)
            print(f"âœ… ä»»åŠ¡ {task_id} å¯åŠ¨æˆåŠŸ")
        except Exception as e:
            print(f"âŒ ä»»åŠ¡ {task.task_id} å¯åŠ¨å¤±è´¥: {e}")
    
    # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
    print(f"\nâ³ ç­‰å¾… {len(task_ids)} ä¸ªä»»åŠ¡å®Œæˆ...")
    
    while task_ids:
        for task_id in task_ids.copy():
            status = await crawler_adapter.get_task_status(task_id)
            if status.get('done'):
                result = await crawler_adapter.get_task_result(task_id)
                print(f"âœ… ä»»åŠ¡ {task_id} å®Œæˆ: {result.success if result else 'Unknown'}")
                task_ids.remove(task_id)
        
        if task_ids:
            await asyncio.sleep(1)


async def test_task_management():
    """æµ‹è¯•ä»»åŠ¡ç®¡ç†åŠŸèƒ½"""
    
    print("\nğŸ“Š æµ‹è¯•ä»»åŠ¡ç®¡ç†åŠŸèƒ½")
    
    # åˆ›å»ºä¸€ä¸ªé•¿æœŸè¿è¡Œçš„ä»»åŠ¡
    task = CrawlerTask(
        task_id="management_test",
        platform=PlatformType.XHS,
        task_type=CrawlerTaskType.SEARCH,
        keywords=["é•¿æœŸä»»åŠ¡æµ‹è¯•"],
        max_count=50,  # è¾ƒå¤§çš„æ•°é‡ï¼Œç¡®ä¿ä»»åŠ¡è¿è¡Œæ—¶é—´è¶³å¤Ÿé•¿
        headless=True,
        save_data_option="json"
    )
    
    try:
        # å¯åŠ¨ä»»åŠ¡
        task_id = await crawler_adapter.start_crawler_task(task)
        print(f"âœ… å¯åŠ¨é•¿æœŸä»»åŠ¡: {task_id}")
        
        # ç­‰å¾…ä¸€æ®µæ—¶é—´
        await asyncio.sleep(5)
        
        # æ£€æŸ¥è¿è¡Œä¸­çš„ä»»åŠ¡åˆ—è¡¨
        running_tasks = await crawler_adapter.list_running_tasks()
        print(f"ğŸ“‹ è¿è¡Œä¸­çš„ä»»åŠ¡: {running_tasks}")
        
        # åœæ­¢ä»»åŠ¡
        success = await crawler_adapter.stop_task(task_id)
        print(f"â¹ï¸ åœæ­¢ä»»åŠ¡ç»“æœ: {success}")
        
        # å†æ¬¡æ£€æŸ¥è¿è¡Œä¸­çš„ä»»åŠ¡
        running_tasks = await crawler_adapter.list_running_tasks()
        print(f"ğŸ“‹ åœæ­¢åè¿è¡Œä¸­çš„ä»»åŠ¡: {running_tasks}")
        
    except Exception as e:
        print(f"âŒ ä»»åŠ¡ç®¡ç†æµ‹è¯•å¤±è´¥: {e}")


def check_mediacrawler_availability():
    """æ£€æŸ¥åŸMediaCrawleré¡¹ç›®æ˜¯å¦å¯ç”¨"""
    
    print("ğŸ” æ£€æŸ¥åŸMediaCrawleré¡¹ç›®...")
    
    mediacrawler_path = os.path.join(
        os.path.dirname(__file__), "..", "MediaCrawler"
    )
    main_py_path = os.path.join(mediacrawler_path, "main.py")
    
    if os.path.exists(main_py_path):
        print(f"âœ… æ‰¾åˆ°åŸMediaCrawler: {main_py_path}")
        return True
    else:
        print(f"âŒ æœªæ‰¾åˆ°åŸMediaCrawler: {main_py_path}")
        print("   è¯·ç¡®ä¿åŸMediaCrawleré¡¹ç›®ä½äºæ­£ç¡®çš„è·¯å¾„")
        return False


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    
    print("=" * 60)
    print("ğŸ§ª MediaCrawleré€‚é…å™¨æµ‹è¯•")
    print("=" * 60)
    
    # æ£€æŸ¥åŸé¡¹ç›®å¯ç”¨æ€§
    if not check_mediacrawler_availability():
        print("\nâš ï¸ è·³è¿‡æµ‹è¯•ï¼Œå› ä¸ºåŸMediaCrawleré¡¹ç›®ä¸å¯ç”¨")
        return
    
    try:
        # å•ä»»åŠ¡æµ‹è¯•
        await test_crawler_adapter()
        
        # å¤šä»»åŠ¡æµ‹è¯•
        await test_multiple_tasks()
        
        # ä»»åŠ¡ç®¡ç†æµ‹è¯•
        await test_task_management()
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main()) 